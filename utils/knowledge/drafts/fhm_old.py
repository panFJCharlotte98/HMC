
fhm_vanilla_def = '''that contains a direct or indirect attack on people based on characteristics, including ethnicity, race, nationality, immigration status, religion, caste, sex, gender identity, sexual orientation, and disability or disease. Such attacks include violent or dehumanizing (comparing people to non-human things, e.g. animals and objects) speech, statements of inferiority, comparison with criminals, mocking hate crime, reinforcing negative stereotypes, use of slurs and calls for exclusion or segregation, etc.'''
FHM_VANILLA_DEF_HATE = f'''A hate speech is any communication {fhm_vanilla_def}'''
FHM_VANILLA_DEF_HATE_MEME = f'''A hateful meme {fhm_vanilla_def}'''
FHM_DEF_BENIGN_MEME = '''A benign meme that is not hateful and follows social norms.'''
FHM_DEF_BENIGN = '''A non-hateful speech that is benign and follows social norms.'''

fhm_modified_def = '''that contains a direct or indirect attack on people or makes fun of people in an inappropriate, disrespectful or offensive manner based on people's characteristics, including ethnicity, race, nationality, immigration status, religion, caste, sex, gender identity, sexual orientation, and disability or disease. Such offensive attacks or jokes include violent or dehumanizing (comparing people to non-human things, e.g. animals and objects) speech, statements of inferiority, comparison with criminals, mocking hate crime, reinforcing negative stereotypes, use of slurs and calls for exclusion or segregation, etc.'''
FHM_DEF_HATE = f'''A hate speech is any communication {fhm_modified_def}'''
FHM_DEF_HATE_MEME = f'''A hateful meme {fhm_modified_def}'''

GL_TARGETS = {
    "Women (female)": "Perpetuating stereotypes related to female's domestic roles (e.g., being in the kitchen, making sandwiches, doing laundry, obeying husbands), objectification of women, comparing women to household appliances, dehumanizing females, suggesting domestic violence towards women, depicting men as superior, and depicting control or manipulation over women, etc.",
"LGBTQ Community": '''Stereotypes that emphasize or attack the appearance of transgender individuals, alienating LGBTQ persons by labeling them as abnormal, denying one's gender identity e.g., misgendering, stigmatization, suggesting violence against LGBTQ individuals, promoting transphobia, homophobia, etc.''',
    "People with Disabilities": '''Offensive and hateful contents that involve mocking, trivializing and making fun of individuals' disabilities, using derogatory, humiliating labels like comparing the disabled to "vegetables", stigmatizing, making puns, punchlines or jokes related to one's disabilities or conditions (such as Down Syndrome), etc.''',
"Muslims and Islamic Culture": '''Stereotypes that involve misrepresentations and disrespectful judgement of Muslim women's attire with prejudice, associating Muslims with terrorism and terrorists, misrepresenting, critisizing or smearing Islamic faith, using offensive animal-related sexual innuendos e.g., implying bestiality, etc.
''',
    "Individuals of Middle Eastern Descent": '''Dehumanizing individuals to animals, comparing humans to animals such as sheep, goats and other livestock or religiously related animals, using offensive animal-related sexual innuendos, associating Middle Eastern people with terrorism and terrorists, etc.''',
"Jewish Individuals": '''Antisemitic contents that make light of the Holocaust, making insensitive and dismissive jokes about violence during Nazi era such as concentration camps, mocking Adolf Hitler's hate crimes, perpetuating stereotypes about Jewish intellectual abilities, mocking or making hurtful jokes about Holocaust victims like Anne Frank, etc.''',
    "Individuals of African Descent": '''Racist contents that compare individuals of African descent to primates (e.g., monkeys, apes or gorillas), making insensitive remarks about segregation, slavery or colonialism (e.g., cotton plantations), dismissing the hurtful impact of historical oppression, etc.''',
"African Americans and other colored people": '''Stereotypes assuming colored people are more likely to engage in criminal activities, linking African Americans with high crime rates, comparing African Americans to primates (e.g., monkeys, apes or gorillas), stereotypes that black children are more likely to grow up in single-parent homes without a father, racist contents that involve mocking skin color, facial features, accents, or intellectual abilities, mocking or questioning colored people's cultural practices, values or ethnic history, etc.''',
    "People of Asian Descent": '''Stereotypes that focus on mocking their facial features, particularly eyes, mocking people's cultural practices e.g., dietary habits, perpetuating offensive caricatures, etc.''',
    "Native Americans (American Indians)": '''mocking their traditions, histories, and spiritual beliefs, stereotypes portraying them as primitive or savage, making light of colonialism and genocide, etc.'''
}

Q4C_1 = "Provide examples of the commonly found stereotypes and the types of offensive, hateful content against"
Q4C_2 = "Provide examples of the commonly found stereotypes and the types of offensive, insensitive, dismissive and hateful content against"
Q4TGCONTEXT  = {
    "Women (female)": f'''{Q4C_1} women (female) in online memes. Only provide phrases without giving detailed explanations, e.g., "objectification of women", "stereotypes related to female's domestic roles", etc.''',
"LGBTQ Community": f'''{Q4C_1} LGBTQ community in online memes. Only provide phrases or terms like "attacking the appearance of transgender individuals", "misgendering", "transphobia", "homophobia", etc. without giving detailed explanations.''',
    "People with Disabilities": f'''{Q4C_1} people of disabilities in online memes. Only provide phrases or terms like "making fun of individuals' disabilities", "using humiliating labels like vegetables", etc. without giving detailed explanations.''',
"Muslims and Islamic Culture": f'''{Q4C_2} Muslims and Islamic culture in online memes. Only provide phrases or terms like "misrepresentations of Muslim women's attire", "associating Muslims with terrorism and terrorists", etc. without giving detailed explanations.''',
    "Individuals of Middle Eastern Descent": f'''{Q4C_2} individuals of Middle Eastern descent in online memes. Only provide phrases or terms like "dehumanizing individuals to animals", "comparing humans to animals", etc. without giving detailed explanations.''',
"Jewish Individuals": f'''{Q4C_2} Jewish Individuals in online memes. Only provide phrases or terms like "making light of the Holocaust and Nazi violence", "mocking Adolf Hitler's hate crimes" etc. without giving detailed explanations.''',
    "Individuals of African Descent": f'''{Q4C_2} individuals of African Descent in online memes. Only provide phrases or terms like "comparing the African descent to primates", "making insensitive remarks about segregation, slavery or colonialism" etc. without giving detailed explanations.''',
"African Americans and other colored people": f'''{Q4C_2} African Americans and other colored people in online memes. Only provide phrases or terms like "linking with criminal activities", "comparing to primates" etc. without giving detailed explanations.''',
    "People of Asian Descent": f'''{Q4C_2} people of Asian Descent in online memes. Only provide phrases or terms like "mocking Asian's facial features, particularly eyes", "mocking Asian's dietary habits" etc. without giving detailed explanations.''',
"Native Americans (American Indians)": f'''{Q4C_2} native Americans (American Indians) in online memes. Only provide phrases or terms like "stereotypes portraying them as primitive or savage" etc. without giving detailed explanations.''',    
    "Others": '''Provide examples of the commonly found stereotypes or the types of offensive, insensitive, dismissive and hateful content against the target group you just identified. Only provide phrases or terms like "mocking one's facial features", etc. without giving detailed explanations.'''
}


protected_groups = ", ".join(list(GL_TARGETS.keys())).strip(", ")
GL1 = f'''1. When analyzing the image, you should only focus on the visual facts. DO NOT suggest any sense of feelings or sentiments implied by the image. DO NOT overinterpret for any implication beyond the image's visual facts.
2. Try to interpret the content by combining both the image and caption as a whole. DO NOT let any single aspect dominate your determination.
3. Try to interpret the implications of the image-caption contents from a neutral perspective without presuming the nature of tone or intent as humorous, playful or lighthearted.
4. If the feeling or sentiment conveyed by the caption does not seem to match the image, review what the content actually means if it is leveraging an irony to convey different implications.
'''

GuideLine_basic = f'''1. Try to interpret the content by combining both the image and caption as a whole. DO NOT let any single aspect dominate your determination.
2. Try to interpret the implications of the image-caption contents from a neutral perspective without presuming the nature of tone or intent as humorous, playful or lighthearted.
3. Some image-caption contents perceived as hateful can be implicit, which means they may not contain explicit derogatory language, offensive speech, or indication of hate towards individuals or groups in the image or caption. However, they may intentionally trigger audience's contextual interpretations with negative associations e.g., harmful stereotypes against protected groups, painful historical events, sensitive controversies surrounding cultural, religious or political practices, etc. thus reinforcing harmful biases, discrimination, and even potential hatefulness against certain human targets.
4. The vulnerable protected groups or individuals within the scope of this task include: {protected_groups}, and those similar to them. Stereotypes and topics surrounding these specific protected groups are the most sensitive and serious, while any other stereotypes or slightly negative implications not concerning these protected groups can be considered as harmless.
5. If the caption merely describes, states, or explains the facts about the image's visual content (e.g., providing context about what is going on in the image) in a neutral tone from an observer's perspective without any sentiment inclination, avoid overinterpreting for negative associations or implications. Such captions, if being objective or illustrative statements, should be considered as innocent.
6. Try to assess the level of hate the content poses to the relevant audience, and how sensitive and serious the involved topic is. Content that only carries a slightly negative implication not targeting any specific protected group or individual might be considered as innocent.
7. Content that leverages offensive plays on words targeting vulnerable protected groups should be considered as hateful.
8. Using derogatory language, mocking, or advocating violence and extremism toward non-human animals is not considered as hateful within the scope of this task. The discussion of hatefulness here pertains only to humans.
9. If the content is neither targeting any protected groups nor likely to cause severely negative impacts, rhetorical metaphor, extreme or exaggeration should not be overinterpretted and might be considered as innocent.
'''

# 1. Try to interpret the content by combining both the image and caption as a whole. DO NOT let any single aspect dominate your determination.
# 2. Try to interpret the implications of the image-caption contents from a neutral perspective without presuming the nature of tone or intent as humorous, playful or lighthearted.
# 3. Some image-caption contents perceived as hateful can be implicit, which means they may not contain explicit derogatory language, offensive speech, or indication of hate towards individuals or groups in the image or caption. However, they may intentionally trigger audience's contextual interpretations with negative associations e.g., harmful stereotypes against protected groups, painful historical events, sensitive controversies surrounding cultural, religious or political practices, etc. thus reinforcing harmful biases, discrimination, and even potential hatefulness against certain human targets.
# 4. The vulnerable protected groups within the scope of this task include: Women (female), LGBTQ Community, People with Disabilities, Muslims and Islamic Culture, Individuals of Middle Eastern Descent, Jewish Individuals, Individuals of African Descent, African Americans and other colored people, People of Asian Descent. Stereotypes and topics surrounding these protected groups are the most sensitive and serious, while any other stereotypes or slightly negative implications not concerning these protected groups can be considered as harmless.
# 5. If the caption merely describes, states, or explains the facts about the image's visual content (e.g., providing context about what is going on in the image) in a neutral tone from an observer's perspective without any sentiment inclination, avoid overinterpreting for negative associations or implications. Such captions, if being objective or illustrative statements, should be considered as innocent.
# 6. Try to assess the level of hate the content poses to the relevant audience, or how sensitive and serious the involved topic is based on common social norms. Content that only carries a slight negative implication not targeting any specific protected group might be considered as innocent.  
# 7. Content that leverages offensive plays on words targeting vulnerable protected groups should be considered as hateful.
# 8. Using derogatory language, mocking, or advocating violence and extremism toward non-human animals is not considered as hateful within the scope of this task. The discussion of hatefulness here pertains only to humans.
# 9. If the content is neither targeting any protected groups nor likely to cause severely negative impacts, rhetorical metaphor, extreme or exaggeration should not be overinterpretted and might be considered as innocent.

# For test_toy
# 1. When analyzing the image, you should only focus on the visual facts. DO NOT suggest any sense of feelings or sentiments implied by the image. DO NOT overinterpret for any implication beyond the image's visual facts.
# 2. Try to interpret the content by combining both the image and caption as a whole. DO NOT let any single aspect dominate your determination.
# 3. Try to interpret the implications of the image-caption contents from a neutral perspective without presuming the nature of tone or intent as humorous, playful or lighthearted.
# 4. If the feeling or sentiment conveyed by the caption does not seem to match the image, review what the content actually means if it is leveraging an irony to convey different implications.
# 3. If the feelings or emotions conveyed by the caption do not seem to align with the feelings evoked by the image, try to examine whether the content might intentionally use an irony or satire to create unexpected twists with the opposite meaning.  
# 4. When the image shows no explicit visual cues indicating any specific human individuals or groups but the caption uses "you", the "you" here should refer to the audience if the content could be holistically perceived as potentially negative or hateful towards the subject referred by "you".
# 4. If the image shows no explicit visual cues indicating any specific human individuals or groups but the caption uses "you", examine how the content might be interpreted if the "you" refers to the audience of the content. If the content could be holistically perceived as potentially negative or hateful towards the subject referred by "you", the "you" here should refer to the audience. 
# 4. If the image shows no explicit visual cues indicating any specific individuals or groups but the caption uses "you", examine how the content might be interpreted if the "you" refers to the audience of the content.

# 3. Some image-caption contents perceived as hateful can be implicit, which means they may not contain explicit derogatory language, offensive speech, or indication of hate towards individuals or groups in the image or caption. However, they may intentionally trigger audience's contextual interpretations with negative associations e.g., harmful stereotypes against protected groups, painful historical events, sensitive controversies surrounding cultural, religious or political practices, etc. thus reinforcing harmful biases, discrimination, and even potential hatefulness against certain human targets.
# 4. The vulnerable protected groups within the scope of this task include: {protected_groups}. Stereotypes and topics surrounding these protected groups are the most sensitive and serious, while any other stereotypes or slightly negative implications not concerning these protected groups can be considered as harmless.
# 5. If the caption merely describes, states, or explains the facts about the image's visual content (e.g., providing context about what is going on in the image) in a neutral tone from an observer's perspective without any sentiment inclination, avoid overinterpreting for negative associations or implications. Such captions, if being objective or illustrative statements, should be considered as innocent.
# 6. Try to assess based on common social norms the level of hate the content poses to the relevant audience, or how sensitive and serious the involved topic is. Content that only carries a slight negative implication not targeting any specific protected group might be considered as innocent.  
# 7. Content that leverages offensive plays on words targeting vulnerable protected groups should be considered as hateful.
# 8. Using derogatory language, mocking, or advocating violence and extremism toward non-human animals is not considered as hateful within the scope of this task. The discussion of hatefulness here pertains only to humans.
# 9. If the content is neither targeting any protected groups nor likely to cause severely negative impacts, rhetorical metaphor, extreme or exaggeration should not be overinterpretted and might be considered as innocent.


# slightly demeaning to Canada. - **Guideline 4**: The caption is not purely descriptive; it includes a pun that carries a slight negative implication

#KNOWLEDGE = f'''{GuideLine_basic}'''

KNOWLEDGE = '''1. Some image-caption contents perceived as hateful can be implicit, which means they may not contain explicit derogatory language, offensive speech, or indication of hate towards individuals or groups in the image or caption. However, they may trigger audience's contextual interpretations with negative associations e.g., stereotypes, painful historical events, sensitive cultural or religious controversies, etc. thus resulting in hatefulness towards certain targets.\n2. Try to interpret the content by combining both the image and caption as a whole. DO NOT let any single aspect dominate your determination.\n3. Try to interpret the implications of the image-caption contents in a neutral tone. DO NOT assume the nature of tone and intent to be humorous, playful or lighthearted. \n4. Be cautious with content that has the potential of trivializing serious or sensitive issues, being dismissive and making light of hate speech by making offensive plays on words.  \n5. Try not to overinterpret if the caption simply describes, states or explains the visual content of the image in a neutral tone as an observer without any sentiment inclination. Such content should be considered innocent if it is just an objective illustrative statement. \n6. Mocking, using derogatory language towards or promoting violence against non-human animals is not considered hateful within the scope of this task. The discussion of hatefulness here pertains only to humans.\n7. As long as the content does not explicitly target any protected groups,rhetorical exaggeration is allowed, even if it might unintentionally evoke negative associations.\n8. Commonly found hateful contents targeting various vulnerable groups include:\n**Women (female)**: Perpetuating stereotypes related to domestic roles, objectification of women, dehumanizing females, comparing women to household appliances, suggesting domestic violence towards women, depicting men as superior, and depicting control or manipulation over women, etc.\n**LGBTQ Community**: Stereotypes that emphasize the appearance of transgender individuals, alienating LGBTQ persons by labeling them as abnormal, denying one's gender identity e.g., misgendering, stigmatization, suggesting violence against LGBTQ individuals, promoting transphobia, and homophobia, etc.\n**People with Disabilities**: Offensive and hateful contents that involve mocking, trivializing and making fun of individuals' disabilities, using derogatory labels like 'vegetables', stigmatizing, making puns, punchlines and jokes related to one's disabilities and conditions (such as Down Syndrome), etc.\n**Muslims and Islamic Culture**: Stereotypes that involve misrepresentations and judging of Muslim women's attire with bias, misrepresenting or smearing Islamic faith, associating Muslims with terrorism and terrorists, and using offensive animal-related sexual innuendos e.g., implying bestiality, etc.\n**Individuals of Middle Eastern Descent**: Dehumanizing individuals to animals, comparing humans to animals such as sheep, goats and other animals that are considered as livestock or religiously related, etc.\n**Individuals of African Descent**: Racist contents that compare individuals of African descent to primates like apes or gorillas, making insensitive remarks about slavery and colonialism (e.g., cotton plantations), etc.\n**Jewish Individuals**: Antisemitic contents that make light of the Holocaust, making insensitive and dismissive jokes about violence during Nazi era such as concentration camps, mocking Adolf Hitler's hate crimes, perpetuating stereotypes about Jewish intellectual abilities, and inappropriate jokes involving figures like Anne Frank.\n**African Americans**: Stereotypes that assume African Americans are more likely to commit crimes/felony, linking African American individuals to high crime rates, comparing African Americans to primates like apes or gorillas, and mocking their skin color and facial features, etc.\n**People of Asian Descent**: Stereotypes that focus on mocking their facial features, particularly eyes, perpetuating offensive caricatures, etc..'''

# 10. Commonly found hateful contents targeting various vulnerable protected groups include:
# **Women (female)**: {GL_TARGETS["Women (female)"]}
# **LGBTQ Community**: {GL_TARGETS["LGBTQ Community"]}
# **People with Disabilities**: {GL_TARGETS["People with Disabilities"]}
# **Muslims and Islamic Culture**: {GL_TARGETS["Muslims and Islamic Culture"]}
# **Individuals of Middle Eastern Descent**: {GL_TARGETS["Individuals of Middle Eastern Descent"]}
# **Jewish Individuals**: {GL_TARGETS["Jewish Individuals"]}
# **Individuals of African Descent**: {GL_TARGETS["Individuals of African Descent"]}
# **African Americans and other colored people**: {GL_TARGETS["African Americans and other colored people"]}
# **People of Asian Descent**: {GL_TARGETS["People of Asian Descent"]}
# **Native Americans (American Indians)**: {GL_TARGETS["Native Americans (American Indians)"]}



# 1. Some image-caption contents perceived as hateful can be implicit, which means they may not contain explicit derogatory language, offensive speech, or indication of hate towards individuals or groups in the image or caption. However, they may intentionally trigger audience's contextual interpretations with negative associations e.g., stereotypes, painful historical events, sensitive controversies surrounding cultural, religious or political practices, etc. thus resulting in hatefulness towards certain targets

#even if it might unintentionally evoke negative associations
# 5. If the caption only states, describes or explains about the visual content of the image in a neutral tone from an observer's perspective without any sentiment or emotion inclination, avoid overinterpreting for negative associations. and examine if it is just an innocent objective or illustrative statement.
# 
# 7. If the content does not involve any protected groups, rhetorical exaggeration should not be overinterpretted and might be taken as innocent depending on the content's impacts on the audience and society.

# 2. Since it is important to acutely detect the potential of hatefulness in these contents, if the image-caption content can be easily associated with such negative factors, the content's intent should be immediately presumed to be hateful rather than trying to beutifying or overinterpreting it for positive implications.


#D6m
# 1. Some image-caption contents perceived as hateful can be implicit, which means they may not contain explicit derogatory language, offensive speech, or indication of hate towards individuals or groups in the image or caption. However, they may intentionally trigger audience's contextual interpretations with negative associations e.g., stereotypes, painful historical events, sensitive controversies surrounding cultural, religious or political practices, etc. thus resulting in potential hatefulness towards certain targets.
# 2. Try to interpret the content by combining both the image and caption as a whole. DO NOT let any single aspect dominate your determination.
# 3. Try to interpret the implications of the image-caption contents from a neutral perspective without presuming the nature of tone or intent as humorous, playful or lighthearted.
# 4. If the caption merely describes, states, or explains the facts about the image's visual content in a neutral tone from an observer's perspective without any sentiment inclination, avoid overinterpreting for negative associations or implications. Such captions, if being objective or illustrative statements, should be considered as innocent.
# 5. Be cautious with content that has the potential of trivializing serious issues, being dismissive of sensitive topics and making light of hate speech towards protected groups through offensive plays on words.
# 6. Mocking, using derogatory language towards or promoting violence against non-human animals is not considered as hateful within the scope of this task. The discussion of hatefulness here pertains only to humans.
# 7. If the content does not involve any protected groups, rhetorical exaggeration might be taken as innocent.


# D6f
# 1. Some image-caption contents perceived as hateful can be implicit, which means they may not contain explicit derogatory language, offensive speech, or indication of hate towards individuals or groups in the image or caption. However, they may trigger audience's contextual interpretations with negative associations e.g., stereotypes, painful historical events, sensitive cultural or religious controversies, etc. thus resulting in hatefulness towards certain targets.
# 2. Try to interpret the content by combining both the image and caption as a whole. DO NOT let any single aspect dominate your determination.
# 3. Try to interpret the implications of the image-caption contents in a neutral tone without presuming the tone as humorous, playful or lighthearted. 
# 4. Be cautious with content that has the potential of trivializing serious issues, being dismissive and making light of hate speech by making offensive plays on words. 
# 5. If the caption simply states or describes the visual content of the image in a neutral tone of an observer without any sentiment inclination, avoid overinterpreting and examine if it is just an innocent objective statement.
# 6. Mocking, using derogatory language towards or promoting violence against non-human animals is not considered hateful within the scope of this task. The discussion of hatefulness here pertains only to humans.
# 7. Commonly found hateful contents targeting various vulnerable groups include:
# **Women (female)**: Perpetuating stereotypes related to domestic roles, objectification of women, dehumanizing females, comparing women to household appliances, suggesting domestic violence towards women, depicting men as superior, and depicting control or manipulation over women, etc.
# **LGBTQ Community**: Stereotypes that emphasize the appearance of transgender individuals, alienating LGBTQ persons by labeling them as abnormal, denying one's gender identity e.g., misgendering, stigmatization, suggesting violence against LGBTQ individuals, promoting transphobia, and homophobia, etc.
# **People with Disabilities**: Offensive and hateful contents that involve mocking, trivializing and making fun of individuals' disabilities, using derogatory labels like 'vegetables', stigmatizing, making puns, punchlines and jokes related to one's disabilities and conditions (such as Down Syndrome), etc.
# **Muslims and Islamic Culture**: Stereotypes that involve misrepresentations and judging of Muslim women's attire with bias, misrepresenting or smearing Islamic faith, associating Muslims with terrorism and terrorists, and using offensive animal-related sexual innuendos e.g., implying bestiality, etc.
# **Individuals of Middle Eastern Descent**: Dehumanizing individuals to animals, comparing humans to animals such as sheep, goats and other animals that are considered as livestock or religiously related, etc.**Individuals of African Descent**: Racist contents that compare individuals of African descent to primates like apes or gorillas, making insensitive remarks about slavery and colonialism (e.g., cotton plantations), etc.
# **Jewish Individuals**: Antisemitic contents that make light of the Holocaust, making insensitive and dismissive jokes about violence during Nazi era such as concentration camps, mocking Adolf Hitler's hate crimes, perpetuating stereotypes about Jewish intellectual abilities, and inappropriate jokes involving figures like Anne Frank.
# **African Americans**: Stereotypes that assume African Americans are more likely to commit crimes/felony, linking African American individuals to high crime rates, comparing African Americans to primates like apes or gorillas, and mocking their skin color and facial features, etc.\n**People of Asian Descent**: Stereotypes that focus on mocking their facial features, particularly eyes, perpetuating offensive caricatures, etc.