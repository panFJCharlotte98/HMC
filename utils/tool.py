import importlib
import regex
import json
import os
import copy
import statistics
from PIL import Image
from utils.knowledge.fhm import FHM_TG_KNOWLEDGE

def get_model(model):
    Model = importlib.import_module('models.{}'.format(model)).Model
    return Model


def get_constructor(constructor):
    Constructor = importlib.import_module('{}'.format(constructor)).Constructor
    return Constructor


def get_evaluator(evaluate_tool):
    EvaluateTool = importlib.import_module('{}'.format(evaluate_tool)).EvaluateTool
    return EvaluateTool


def extract_json_output(text, keys=None):
    '''
    input text is the direct output generated by LLMs/LMMs.
    something like "Hi, sure I can help you. json in string."
    '''
    text = " ".join(text.strip("\"``` ").replace("\n", " ").replace(" :", ":")\
                    .replace(": ", ":").replace('" ', '"').replace(' "', '"')\
                    .replace('\\\"', "*").replace("\\'", "'").split())
    text = regex.sub(r"\}.*\}", "}", regex.sub(r"\{.*\{", "{", text)).strip()
    this_key = ""
    all_possible_keys = keys if keys is not None else ['description', 'output']
    for key in all_possible_keys:
        if "\"{key}\":".format(key=key) in text:
            this_key = key
            # add left quote if missing
            if "\"{key}\":\"".format(key=key) not in text:
                text = text.replace("\"{key}\":".format(key=key), "\"{key}\":\"".format(key=key))
            # add right quote and right curled bracket if missing
            if '}' in text:
                if "\"}" not in text:
                    text = text.replace('}', "\"}")
            else:
                text = text.rstrip("'\"") + "\"}"
            value = text.split("\"{key}\":\"".format(key=key))[1].split("\"}")[0]
            clean_value = value.replace("\"", "*").strip("'")
            text = text.replace(value, clean_value)
            break
    if this_key:
        try:
            js_obj = json.loads(regex.findall(r"\{.*\}", text)[0])
        except:
            return 'failed_to_extract_json'
        if keys is None:
            return js_obj[this_key]
        else:
            return js_obj
    else:
        if any([possible_label in text for possible_label in ['non-hateful', 'not hateful']]):
            return {"decision": "non-hateful"}
        else:
            if "hateful" in text:
                return {"decision": "hateful"}
            else:
                return 'gen_error'


def extract_yes_or_no(text):
    flag = 999
    try:
        lower_text = text.lower()
        words = lower_text.split()
        first_word = words[0].strip(''':/,."' ''')
        if first_word.startswith('yes'): 
            # "Yes, there is ....""
            flag = 1
        if first_word.startswith('no'):
            flag = 0
        if flag == 999:
            output = text
        else:
            # strip the first word
            if len(text.split()) > 1:
                word_ls = text.split()[1:]
                word_ls[0] = word_ls[0].capitalize()
                output = " ".join(word_ls).strip(''':/,' ''')
            else:
                output = ""
    except:
        output = text
    return {'flag': flag, 'output': output}

def extract_celeb(text):
    if "i can't tell" in text.lower():
        return ""
    else:
        return text

def extract_disable(text):
    output = ""
    word1 = text.split()[0].lower().strip(",.;")
    if word1.startswith("yes"):
        word_ls = text.split()[1:]
        word_ls[0] = word_ls[0].capitalize()
        output = " ".join(word_ls).strip(''':/,' ''')
        flag = 1
    else:
        if word1.startswith("no"):
            flag = 0
        else:
            no_conds = text.lower().startswith("there is no ") or text.lower().startswith("the image does not ")
            if no_conds:
                flag = 0
            else:
                flag = 1
                output = text
    return {'flag': flag, 'output': output}

def extract_classification_label_yes_or_no(text):
    pred_label = 404
    
    lower_text = text.lower()
    words = lower_text.split()
    first_word = words[0].strip(''':/,."' ''')
    if first_word.startswith('yes'): 
        # "Yes.""
        pred_label = 1
    if first_word.startswith('no'):
        pred_label = 0
    if pred_label != 404:
        return {'pred_label': pred_label}
    return pred_label


def harmc_extract_classification_label(text):
    ltext = text.lower()
    regs = {
        'harmful_conclusion': r"conclusion[\"\:\*\s]*(?:[a-z]*\s)?harmful",
        'harmless_conclusion': r"conclusion[\"\:\*\s]*(?:[a-z]*\s)?harmless",
        'non-harmful_conclusion': r"conclusion[\"\:\*\s]*(?:[a-z]*\s)?non\-harmful",
    }
    for k, reg in regs.items():
        reg_obj = regex.compile(reg)
        res_ls = reg_obj.findall(ltext)
        if len(res_ls) > 0:
            pred_label = 0
            lb = k.split("_")[0]
            if lb == 'harmful':
                pred_label = 1
            return {'pred_label': pred_label, 'ori_pred_label': res_ls[0]}
    # No expected classification label found, check "I cannot" pattern
    if ltext.startswith("i cannot") or ltext.startswith("i can't"):
        return {'pred_label': 1}
    # No expected classification label found
    return 404


def mami_extract_classification_label(text):
    ltext = text.lower()
    regs = {
        'miso_conclusion': r"conclusion[\"\:\*\s]*(?:[a-z]*\s)?misogynistic",
        'nonmiso_conclusion': r"conclusion[\"\:\*\s]*(?:[a-z]*\s)?non\-misogynistic",
    }
    for k, reg in regs.items():
        reg_obj = regex.compile(reg)
        res_ls = reg_obj.findall(ltext)
        if len(res_ls) > 0:
            pred_label = 0
            lb = k.split("_")[0]
            if lb == 'miso':
                pred_label = 1
            return {'pred_label': pred_label, 'ori_pred_text': res_ls[0]}
    # No expected classification label found, check "I cannot" pattern
    if text.startswith("I cannot") or text.startswith("I can't"):
        return {'pred_label': 1}
    # No expected classification label found
    return 404


def mami_extract_stage_decision_label(text):
    ltext = text.lower()
    regs = {
        'miso_conclusion': r"conclusion[\"\:\*\s]*(?:[a-z]*\s)?misogynistic\s*\(?\s*(?:[a-z\-]*\s?)*\s*\)?",
        'nonmiso_conclusion': r"conclusion[\"\:\*\s]*(?:[a-z]*\s)?non\-misogynistic",
        'noevidence_conclusion': r"conclusion[\"\:\*\s]*(?:[a-z]*\s)?no\sevidence",
    }
    for k, reg in regs.items():
        reg_obj = regex.compile(reg)
        res_ls = reg_obj.findall(ltext)
        if len(res_ls) > 0:
            pred_label = 2 #no evidence
            lb = k.split("_")[0]
            if lb == 'miso':
                pred_label = 1
            if lb == 'nonmiso':
                pred_label = 0
            return {'pred_label': pred_label, 'ori_pred_text': res_ls[0]}
    # No expected classification label found, check "I cannot" pattern
    if text.startswith("I cannot") or text.startswith("I can't"):
        return {'pred_label': 1}
    # No expected classification label found
    return 404

class MAMI_Extractor(object):
    def __init__(self, miso_type):
        self.type = miso_type.lower()
        self.regs = {
            'miso_conclusion': r"conclusion[\"\:\*\s]*(?:[a-z]*\s)?misogynistic\s*\(?\s*(?:[a-z\-]*\s?)*\s*\)?",
            'noevidence_conclusion': r"conclusion[\"\:\*\s]*(?:[a-z]*\s)?no\sevidence",
            'nonmiso_conclusion': r"conclusion[\"\:\*\s]*(?:[a-z]*\s)?non\-misogynistic",
        }
    def mami_extract_stage_decision_label(self, text):
        ltext = text.lower()
        for k, reg in self.regs.items():
            reg_obj = regex.compile(reg)
            res_ls = reg_obj.findall(ltext)
            if len(res_ls) > 0:
                pred_label = 3 # no evidence
                lb = k.split("_")[0]
                if lb == 'nonmiso':
                    pred_label = 0
                if lb == 'miso':
                    pred_label = 2 # cls as miso, but not this type
                    if all([wd in res_ls[0] for wd in self.type.split()]):
                        pred_label = 1 # cls as miso, and this type
                return {'pred_label': pred_label, 'ori_pred_text': res_ls[0]}
        # No expected classification label found, check "I cannot" pattern
        if text.startswith("I cannot") or text.startswith("I can't"):
            return {'pred_label': 2}
        # No expected classification label found
        return 404
    
def mami_extract_step_reasons(text):
    '''
    "1. Harmful Stereotypes; 2. Body Shaming; 3. Objectification of Women; 4. Sexualization of Women; 5. Suggesting Violence Towards Women; 6. Anti-Feminist/Anti-Feminism."
    "1. NO evidence suggesting Guideline 3; 2. Application of Guideline 4."
    '''
    reasons = []
    ltext = text.lower()
    keyword_map = {
        "stereotypes": ["1. harmful stereotypes"],#['stereotype'],
        "body shaming": [". body shaming"],#['body sham'],
        "objectification": [". objectification of"],#['object'],
        "sexualization": [". sexualization of "],#['sexualiz'],
        "violence": ["5. suggesting violence towards"],#["violen"],
        "anti-feminist": ["6. anti-feminis"],#["anti-feminis"],
        "no evidence3": ["no evidence suggesting guideline 3"],#['no evidence', '3'],
        "no evidence4": ["no evidence suggesting guideline 4"],#['no evidence', '4'],
        "gl4": ["application of guideline 4"]#['appl', '4']
    }
    for kw, kw_ls in keyword_map.items():
        if all([w in ltext for w in kw_ls]):
            reasons.append(kw)
    return reasons

def extract_classification_label(text):
    ltext = text.lower()
    regs = {
        'hateful_conclusion': r"conclusion[\"\:\*\s]*(?:[a-z]*\s)?hateful",
        'non-hateful_conclusion': r"conclusion[\"\:\*\s]*(?:[a-z]*\s)?non\-hateful",
        
        'harmful_conclusion': r"conclusion[\"\:\*\s]*(?:[a-z]*\s)?harmful",
        'harmless_conclusion': r"conclusion[\"\:\*\s]*(?:[a-z]*\s)?harmless",

        'hurtful_conclusion': r"conclusion[\"\:\*\s]*(?:[a-z]*\s)?hurtful",
        'non-hurtful_conclusion': r"conclusion[\"\:\*\s]*(?:[a-z]*\s)?non\-hurtful",

        'offensive_conclusion': r"conclusion[\"\:\*\s]*(?:[a-z]*\s)?offensive",
        'non-offensive_conclusion': r"conclusion[\"\:\*\s]*(?:[a-z]*\s)?non\-offensive",

        'yes_conclusion': r"conclusion[\"\:\*\s]*(?:[a-z]*\s)?yes",
        'no_conclusion': r"conclusion[\"\:\*\s]*(?:[a-z]*\s)?no",
    }
    for k, reg in regs.items():
        reg_obj = regex.compile(reg)
        res_ls = reg_obj.findall(ltext)
        if len(res_ls) > 0:
            pred_label = 0
            lb = k.split("_")[0]
            if lb in ['hateful', 'hurtful', 'harmful', 'offensive', 'yes']:
                pred_label = 1
            return {'pred_label': pred_label, 'ori_pred_label': res_ls[0]}
    
    # No expected classification label found, check "I cannot" pattern
    if text.startswith("I cannot") or text.startswith("I can't"):
        return {'pred_label': 1}
    if "I can't assist" in text:
        return {'pred_label': 1}
    # No expected classification label found
    return 404


def fhm_extract_target_group(text):
    ltext =  text.lower().replace("conclusion", "").strip(":*. ")
    if "No specific protected group involved.".lower() in ltext:
        return []
    ltext_words = [w.strip('''(*)\'".;,-#''') for w in ltext.split()]
    target_groups = []
    # keyword_map = {
    #     "Women (female)": ["women", "female"],
    #     "LGBTQ Community" : ["lgbtq", "trans", "homosexual"],
    #     "People with Disabilities": ["disabilities", "disabled", "syndrome", "disability"],
    #     "Muslims and Islamic Culture" : ["muslim", "islam"],
    #     "Individuals of Middle Eastern Descent": ["middle east"],
    #     "Jewish Individuals" : ["jewish", "jew"],
    #     "Individuals of African Descent" : ["african"],
    #     "African Americans and other colored people": ["african american", "color", "african"],
    #     "People of Asian Descent": ["asian", "asia"],
    #     "Native Americans (American Indians)": ['native american', 'american indian'],
    #     "Others" : ["other", "not listed"]
    # }
    # for group_name, kw_ls in keyword_map.items():
    #     for kw in kw_ls:
    #         if len(kw.split()) > 1:
    #             if kw in ltext:
    #                 target_groups.append(group_name)
    #         if kw in ltext_words:
    #             target_groups.append(group_name)
    #             break

    for abbr, card in FHM_TG_KNOWLEDGE.items():
        for kw in card['kws']:
            if len(kw.split()) > 1:
                if (kw in ltext) and (abbr not in target_groups):
                    target_groups.append(abbr)
                    break
            else:
                if regex.match(r"\d", kw):
                    if (kw in ltext_words) and (abbr not in target_groups):
                        target_groups.append(abbr)
                        break
                else:
                    if any([kw in lword for lword in ltext_words]) and (abbr not in target_groups):
                        target_groups.append(abbr)
                        break
    #target_groups = list(set(target_groups))
    return target_groups

#-------------------------------- PrideMM----------------------------------#
def pridemm_extract_stance(text):
    ltext = text.lower()
    lwords = [w.strip(".*,#") for w in ltext.split()]
    keyword_map = {
        "neutral": ['1'],
        "support": ['2'],
        "oppose": ['3'],
    }
    for kw, kw_ls in keyword_map.items():
        if kw in ltext:
            return kw
        for kw_ in kw_ls:
            if any([kw_ in w for w in lwords]):
                return kw
    return "neutral"

def pridemm_extract_target(text):
    ltext = text.lower()
    lwords = [w.strip(".*,#") for w in ltext.split()]
    keyword_map = {
        "undirected": ['1'],
        "individual": ['2'],
        "lgbtq": ['3', 'community'],
        "organization" : ['4'],
    }
    for kw, kw_ls in keyword_map.items():
        if kw in ltext:
            return kw
        for kw_ in kw_ls:
            if any([kw_ in w for w in lwords]):
                return kw
    return None

def pridemm_extract_subgroup(text):
    ltext = " ".join(text.lower().replace("conclusion", "").strip(":*. ").split())
    lwords = [w.strip(".*,#") for w in ltext.split()]
    subgroup = []
    keyword_map = {
        'Gay': ['gay', 'lesbian', '1'],
        'Trans women': ['trans women', 'trans female', 'trans girl', 'transwomen', 'transwoman', 'transgender women', 'transfeminine', '2'],
        'Trans men': ['trans men', 'transman', 'trans man', 'transmen', 'transmasc', '3'],
        "(Semi-) Bisexual individuals" : ['bisexual', 'bi-sexual','semi-bisexual', '4'],
        "Non-binary individuals": ['non-binary', 'non binary', '5'],
        "Other LGBTQ+ subgroups": ['other subgroup', 'other lgbtq+', '6'], 
    }
    for subg, kw_ls in keyword_map.items():
        for kw in kw_ls:
            if (len(kw.split()) > 1):
                if (kw in ltext):
                    subgroup.append(subg)
                    break
            else:
                if any([kw in w for w in lwords]):
                    subgroup.append(subg)
                    break
    return subgroup

def pridemm_extract_target_group(text):
    ltext =  text.lower().replace("conclusion", "").strip(":*. ")
    if "no specific sociocultural group involved".lower() in ltext:
        return []
    ltext_words = [w.strip(",;./") for w in ltext.split()]
    if all([w in ltext_words for w in "no sociocultural group involved".split()]):
        return []
    target_groups = []
    keyword_map = {
        "Gay" : ['gay'],
        "Transgender individuals": ['transgender', 'trans'],
        "(Semi-) Bisexual individuals" : ['bisexual', 'bi-sexual', 'semi-bisexual', 'semi'], 
        "Other LGBTQ+ groups": ['other lgbtq'], 
        "LGBTQ+ community as a whole": ['as a whole', 'lgbtq+ community'],
        "Women (Female)": ["women", "female"],
        "Corporations": ["corporations", "female"],
        "Streaming media platforms": ['streaming', 'platforms'],
        "Goverment": ['goverment'],
        "Political ideologies or parties": ['political'],
        "Religions": ['religions', 'religio'],
        "Countries or regions": ['countries', 'country', 'regions'],
        "Celebrities": ['celeb'], 
        "Other sociocultural groups not listed": ["other sociocultural group"]
    }
    for group_name, kw_ls in keyword_map.items():
        for kw in kw_ls:
            if len(kw.split()) > 1:
                if kw in ltext:
                    target_groups.append(group_name)
            else:
                if (kw in ltext_words):
                    target_groups.append(group_name)
                else:
                    if kw in ltext:
                        target_groups.append(group_name)
                break
    target_groups = list(set(target_groups))
    return target_groups


def pridemm_extract_lgbt_subgroup(text):
    ltext =  text.lower().replace("conclusion", "").strip(":*. ")
    no_tmp = "no specific subgroup mentioned"
    if no_tmp.lower() in ltext:
        return []
    ltext_words = [w.strip(",;./") for w in ltext.split()]
    if all([w in ltext_words for w in "no subgroup".split()]):
        return []
    target_groups = []
    keyword_map = {
        "Gay" : [1, 'gay'],
        "Transgender": [2, 'transgender', 'trans'],
        "(Semi-) Bisexual" : [3, 'bisexual', 'bi-sexual', 'semi-bisexual', 'semi'], 
        "Others": [4, 'other']
    }
    for group_name, kw_ls in keyword_map.items():
        for kw in kw_ls:
            if isinstance(kw, int):
                if str(kw) in ltext_words:
                    target_groups.append(group_name)
            else:
                if len(kw.split()) > 1:
                    if kw in ltext:
                        target_groups.append(group_name)
                        break
                else:
                    if any([kw in word for word in ltext_words]):
                        target_groups.append(group_name)
                        break
    target_groups = list(set(target_groups))
    return target_groups

def pridemm_extract_entity(text):
    ltext =  text.lower().replace("conclusion", "").strip(":*. ")
    no_tmp = "no specific sociocultural entity mentioned"
    if no_tmp.lower() in ltext:
        return []
    ltext_words = [w.strip(",;./") for w in ltext.split()]
    if all([w in ltext_words for w in "no entity mentioned".split()]):
        return []
    target_groups = []
    keyword_map = {
        "Women (Female)": [1, "women", "female"],
        "Children": [2, "children", "kids"],
        "Corporations": [3, "corporations"],
        "Streaming media platforms": [4, 'streaming', 'platforms'],
        "Goverment": [5, 'goverment'],
        "Political ideologies or parties": [6, 'political', 'politics'],
        "Religions": [7, 'religions', 'religio'],
        "Countries or regions": [8, 'countries', 'country', 'regions'],
        "Celebrities": [9, 'celeb'], 
        "Other sociocultural groups not listed": [10, "other sociocultural entit", "other entit"]
    }
    for group_name, kw_ls in keyword_map.items():
        for kw in kw_ls:
            if isinstance(kw, int):
                if str(kw) in ltext_words:
                    target_groups.append(group_name)
            else:
                if len(kw.split()) > 1:
                    if kw in ltext:
                        target_groups.append(group_name)
                        break
                else:
                    if any([kw in word for word in ltext_words]):
                        target_groups.append(group_name)
                        break
    target_groups = list(set(target_groups))
    return target_groups

def post_process_to_remove_gibberish(text):
    ori_text = text
    sub_map = {
        r"\\u2019": "'",
        r"\\u201c": "'",
        r"\\u201d": "'",
        r"\\'": "'",
        r"(?:(?:\\u[a-z0-9]{4}){1,}[\s\-\-\*0-9\.\']*){1,}": ""
    }
    text = ascii(text)
    for re, sub_str in sub_map.items():
        text = regex.sub(re, sub_str, text)
    text = text.strip("' ")
    con1 = (not ori_text.startswith("\"")) and (text.startswith("\"")) and (text.endswith('"'))
    con2 = (text.startswith('"')) and (text.endswith('"'))
    if con1 or con2:
        # quan = text.count('"')
        text = text[1:-1]
    return text

def post_process_visual_describe(text):
    text = post_process_to_remove_gibberish(text)
    rm_words = ['humorous', 'humorously', 'playfully', 'playful', 'lighthearted', 'light-hearted']
    for w in rm_words:
        text = text.replace(w, "")
    text = " ".join(text.split())
    return text

def post_process_description(text):
    text = post_process_to_remove_gibberish(text)
    rm_words = ['humorously', 'playfully']
    for w in rm_words:
        text = text.replace(w, "")
    text = " ".join(text.split())
    return text

def extract_target_group_context(js, text, use_ori=False):
    if use_ori and text:
        pattern = regex.compile(r"[1\.\-]{1,}\s?\*{1,}")
        if len(pattern.findall(text)) > 0:
            the_first_pattern = pattern.findall(text)[0]
            # #print(type(the_first_pattern))
            text = the_first_pattern + the_first_pattern.join(text.split(the_first_pattern)[1:])
            return text.lstrip(". ")
        else:
            return text
    if (not use_ori) and text:
        text = text.replace("Summary of", "").replace(" in Online Memes", "").replace("Hateful Contents", "").replace("Hateful Content", "")
        text = " ".join(text.split())
        word_ls = text.split()
        fist_word_l = word_ls[0].lower()
        if (fist_word_l.startswith("certainly")) or (fist_word_l.startswith("sure")):
            text = " ".join(word_ls[1:])
        #pattern = regex.compile(r"[hH]ere\sis(?:[a-zA-Z]*\s)*list(?:[a-zA-Z]*\s)*(?:[a-zA-Z]*)\s?\:")
        # if all([tk in text for tk in ["### ", "#### "]]):
        #     text = "#### " + " #### ".join([para.strip() for para in text.split("####")[1:]])
        #     return text
        r1 = r"\#{1,}\s?(?:[^#:]){1,}:\s*(?:[^#]){2,}"
        L1 = regex.findall(r1, text)
        if L1:
            text = " ".join(L1).replace("### Summary:", "")
            return text.lstrip(". ")
        else:
            pattern = regex.compile(r"[1\.\-]{1,}\s?\*{1,}")
            if len(pattern.findall(text)) > 0:
                the_first_pattern = pattern.findall(text)[0]
                # #print(type(the_first_pattern))
                text = the_first_pattern + the_first_pattern.join(text.split(the_first_pattern)[1:])
                return text.lstrip(". ")
            else:
                pattern = regex.compile(r"include(?:s)?\:")
                if len(pattern.findall(text)) == 1:
                    text = text.split("include:")[-1].strip()
                return text
        return text

def post_process_gen_target_group_context(js, text):
    assert js["TargetGroup"]
    tg_ls = [tg for tg in js["TargetGroup"]]
    n_tg = len(tg_ls)
    if n_tg > 1:
        plh = "#####"
        index_dot = regex.findall(r"\d+\.", text)
        cond1 = (len(index_dot) > n_tg)
        cond2 = (len(index_dot) == n_tg)
        cond3 = (len(index_dot) == n_tg - 1)
        cond4 = len(index_dot) == 0
        if cond1:
            try:
                #print(js["id"])
                assert regex.findall(r"\#+", text)
                pat1s = regex.findall(r"\#+\s?(?:\d+\.?)?(?:[^\d])*", text)
                assert pat1s
                for pat1 in pat1s:
                    text = text.replace(pat1, plh)
                examples = [regex.sub(r"\d+\.", ";", piece) for piece in text.split(plh) if piece]
                new_examples = []
                for exp in examples:
                    new_exp = "; ".join([e.strip() for e in exp.split(";") if e]) + ";"
                    new_examples.append(new_exp)
            except:
                cond2 = True
        if cond2 or cond3 or cond4:
            print(js["id"])
            #assert len(regex.findall(r"\-\s", text)) >= 2
            if regex.findall(r"\#+", text):
                for pat2 in regex.findall(r"\#+\s?(?:\d+\.?)?(?:[^\-\d])*", text):
                    text = text.replace(pat2, f"- {pat2}")
            else:
                for pat3 in regex.findall(r"\d+\s?\.?(?:[^\-])*", text):
                    text = text.replace(pat3, f"- {pat3}")
            tmp, examples = [], []
            for piece in text.split("- "):
                if piece:
                    if regex.match(r"^\#+", piece) or regex.match(r"^\d+\.", piece):
                        if tmp:
                            examples.append(tmp)
                        tmp = []
                    else:
                        piece = piece.strip("# ")
                        # if piece.startswith('"') and piece.endswith('"'):
                        #     piece = piece[1:-1]
                        tmp.append(piece)
            if tmp:
                examples.append(tmp)
            new_examples = []
            for tmp in examples:
                new_examples.append("; ".join(tmp)+";")
        #print(js["id"])
    else:
        assert regex.findall(r"\d+\.", text) or regex.findall(r"\-\s", text)
        if regex.findall(r"\d+\.", text):
            text = regex.sub(r"\d+\.", "- ", text)
        if regex.findall(r"\-\s", text):
            ptext = "; ".join([p.strip("#; ") for p in text.split("- ") if p]) + ";"
        new_examples = [ptext]
    #print(js["id"])
    return new_examples

# def post_process_gen_target_group_context(js, text):
#     if "- " in text:
#         assert "- " in text
#         # pieces = [p.strip('" ') for p in text.split("- ") if not p.strip().startswith("###")]
#         tmp, examples = [], []
#         patterns = [r"\#+\s?(?:[^-])*", r'\d+\s?\.?(?:[^\-])*']
#         for p in text.split("- "):
#             p = p.strip('" ').replace("-", "^")
#             pat_ls = []
#             for pat in patterns:
#                 pat_ls.extend(regex.findall(pat, p))
#             if pat_ls:
#                 for p_match in pat_ls:
#                     p = p.replace(p_match, "").strip('" ')
#                 if p:
#                     p = p.replace("^", "-").strip('" ')
#                     tmp.append(p)
#                 if tmp:
#                     examples.append(tmp)
#                     tmp = []
#             else:
#                 p = p.replace("^", "-").strip('" ')
#                 tmp.append(p)  
#         if tmp:
#             examples.append(tmp)
#         new_examples = []
#         for exp_ls in examples:
#             exp_ls = [exp.strip('"#; ') for exp in exp_ls]
#             t = "; ".join(exp_ls)+";"
#             new_examples.append(t.lstrip("; "))
#     else:
#         new_examples = [text]
#     return new_examples
#     # tmp, examples = [], []
#     # for p in pieces:
#     #     if p:
#     #         if "##" in p:
#     #             new_p = regex.sub(r"\#{1,}\s?(?:[^:])*:", "", p).strip()
#     #             tmp.append(new_p)
#     #             examples.append("; ".join(tmp)+";")
#     #             tmp = []
#     #         else:
#     #             tmp.append(p)
#     # if tmp:
#     #     examples.append("; ".join(tmp)+";")
#     # return examples


def multioff_extract_classification_label(text):
    ltext = text.lower()
    regs = {
        'off_conclusion': r"conclusion[\"\:\*\s]*(?:[a-z]*\s)?offensive",
        'nonoff_conclusion': r"conclusion[\"\:\*\s]*(?:[a-z]*\s)?non\-offensive"
    }
    for k, reg in regs.items():
        reg_obj = regex.compile(reg)
        res_ls = reg_obj.findall(ltext)
        if len(res_ls) > 0:
            pred_label = 0
            lb = k.split("_")[0]
            if lb == 'off':
                pred_label = 1
            # if lb == 'nonoff':
            return {'pred_label': pred_label, 'ori_pred_text': res_ls[0]}
    # No expected classification label found, check "I cannot" pattern
    if text.startswith("I cannot") or text.startswith("I can't"):
        return {'pred_label': 1}
    # No expected classification label found
    return 404


def resize_image_(image_path, save_dir, max_size_mb=20, min_size_mb=0.12):
    """
    Compress image if it's larger than max_size_mb while maintaining quality
    
    Args:
        image_path (str): Path to input image
        max_size_mb (int): Maximum file size in MB
    Returns:
        str: Path to compressed image
    """
    # Get file size in MB
    file_size = os.path.getsize(image_path) / (1024 * 1024)
    
    ori_file_size = copy.deepcopy(file_size)

    if (file_size >= min_size_mb) and (file_size <= max_size_mb):
        return image_path
       
    # Open image
    img = Image.open(image_path)
    
    # Get original format
    original_format = img.format if img.format else 'JPEG'
    if file_size > max_size_mb:
        op = "Compressed"
    elif file_size < min_size_mb:
        op = "Enlarged"
    output_name = image_path.split("/")[-1].split(".")[0] + f"_{op.lower()}." + original_format.lower()
    output_path = os.path.join(save_dir, output_name)
    
    if not os.path.exists(output_path):
        if file_size > max_size_mb:
            # Initial quality
            quality = 95
            while file_size > max_size_mb and quality > 5:
                img.save(output_path, original_format, quality=quality)
                file_size = os.path.getsize(output_path) / (1024 * 1024)
                quality -= 5
        elif file_size < min_size_mb:
            # Enlarge image
            width, height = img.size
            scale_factor = 1.1  # Increase by 10% each iteration
            while file_size < min_size_mb:
                new_width = int(width * scale_factor)
                new_height = int(height * scale_factor)
                img = img.resize((new_width, new_height), Image.LANCZOS)
                img.save(output_path, original_format)
                file_size = os.path.getsize(output_path) / (1024 * 1024)
                width, height = new_width, new_height
    else:
        file_size = os.path.getsize(output_path) / (1024 * 1024)

    #print(f"{op} {image_path} from {ori_file_size}MB to {file_size}MB.")    
    return output_path


def resize_image(
    image_path, 
    save_dir, 
    max_size_mb=None, 
    min_size_mb=None,
    min_len=512,
    max_len=10240
):
    """
    Compress image if it's larger than max_size_mb while maintaining quality
    
    Args:
        image_path (str): Path to input image
        max_size_mb (int): Maximum file size in MB
    Returns:
        str: Path to compressed image
    """
    # Open image
    img = Image.open(image_path)
    # Get original format
    original_format = img.format if img.format else 'JPEG'
    if max_size_mb and min_size_mb:
        # Get file size in MB
        file_size = os.path.getsize(image_path) / (1024 * 1024)
        ori_file_size = copy.deepcopy(file_size)

        if (file_size >= min_size_mb) and (file_size <= max_size_mb):
            return image_path
        if file_size > max_size_mb:
            op = "Compressed"
        elif file_size < min_size_mb:
            op = "Enlarged"
        output_name = image_path.split("/")[-1].split(".")[0] + f"_{op.lower()}." + original_format.lower()
        output_path = os.path.join(save_dir, output_name)
        
        if not os.path.exists(output_path):
            if file_size > max_size_mb:
                # Initial quality
                quality = 95
                while file_size > max_size_mb and quality > 5:
                    img.save(output_path, original_format, quality=quality)
                    file_size = os.path.getsize(output_path) / (1024 * 1024)
                    quality -= 5
            elif file_size < min_size_mb:
                # Enlarge image
                width, height = img.size
                scale_factor = 1.1  # Increase by 10% each iteration
                while file_size < min_size_mb:
                    new_width = int(width * scale_factor)
                    new_height = int(height * scale_factor)
                    img = img.resize((new_width, new_height), Image.LANCZOS)
                    img.save(output_path, original_format)
                    file_size = os.path.getsize(output_path) / (1024 * 1024)
                    width, height = new_width, new_height
        else:
            file_size = os.path.getsize(output_path) / (1024 * 1024)

        print(f"{op} {image_path} from {ori_file_size}MB to {file_size}MB.")    
        return output_path
    
    if min_len:
        # Get image dimensions
        width, height = img.size
        long_edge = max(width, height)
        if (long_edge >= min_len) and (long_edge <= max_len):
            return image_path
        if long_edge < min_len:
            op = "Enlarged"
            target_len = min_len
        elif long_edge > max_len:
            op = "Compressed"
            target_len = max_len
        output_name = image_path.split("/")[-1].split(".")[0] + f"_{op.lower()}." + original_format.lower()
        output_path = os.path.join(save_dir, output_name)
        
        if not os.path.exists(output_path):
            assert (target_len > 0) and (target_len < 10240)
            # Calculate scale factor to reach target_long_edge
            scale_factor = target_len / long_edge
            # Calculate new dimensions while maintaining aspect ratio
            new_width = int(width * scale_factor)
            new_height = int(height * scale_factor)
            # Resize image
            img = img.resize((new_width, new_height), Image.LANCZOS)
            img.save(output_path, original_format)
        else:
            img = Image.open(output_path)
        resized_w, resized_h = img.size
        print(f"{op} {image_path} from {height}x{width} to {resized_h}x{resized_w}.")    
        return output_path


def preprocess_resize_img(
    processed_image_paths,
    ori_folder_name,
    split,
    new_split_image_path,
    new_img_path,
    item,
    after_sizes,
    min_len,
    default_max_l = 102400,
):
    # # Check if need to enlarge or compress images
    for max_l, tup in processed_image_paths.items():
        p_folder = tup[0]
        # processed_image_path = tup[1]
        # p_save_dir = os.path.join(processed_image_path, img_dir, split, label_folder)
        p_save_dir = new_split_image_path.replace(ori_folder_name, p_folder)
        
        if not os.path.exists(p_save_dir):
            os.makedirs(p_save_dir)
        p_img_path = resize_image(
            new_img_path, 
            p_save_dir,
            min_len=min_len,
            max_len=max_l
        )
        if p_img_path != new_img_path:
            key = f'img_{min_len}'
            if max_l < default_max_l:
                key += f"_{max_l}"
            # #print(p_img_path)
            # item[key] = os.path.join('./data/HarMeme_V1', p_folder, img_dir, split, label_folder, p_img_path.split("/")[-1])
            ori_file_name = new_img_path.split("/")[-1]
            resized_file_name = p_img_path.split("/")[-1]
            item[key] = item['img'].replace(ori_folder_name, p_folder).replace(ori_file_name, resized_file_name)

        img = Image.open(p_img_path)
        after_sizes[max_l][split]['w'][item['id']], after_sizes[max_l][split]['h'][item['id']] = img.size
    
    return item, after_sizes


def view_img_size_statistics(
    tmp,
    split,
    dataset_size,
    data_name,
):
    for k, sizes in tmp.items():
        wh_values = []
        if k == 'before':
            width_values, height_values = list(sizes[split]['w'].values()), list(sizes[split]['h'].values())
            wh_values.append((width_values, height_values))
        if (k == 'after') and sizes:
            for max_l, max_l_sizes in sizes.items():
                width_values, height_values = list(max_l_sizes[split]['w'].values()), list(max_l_sizes[split]['h'].values())
                wh_values.append((width_values, height_values))
        for vals in wh_values:
            width_values = vals[0]
            height_values = vals[1]
            if width_values and height_values:
                print(f"{k}|{data_name}: {split}| #{dataset_size} images in total \n Width: min: {min(width_values)}, max: {max(width_values)}, mean: {statistics.mean(width_values)}, median: {statistics.median(width_values)} \n Height: min: {min(height_values)}, max: {max(height_values)}, mean: {statistics.mean(height_values)}, median: {statistics.median(height_values)}")
    return

# POST_PROCESS_FUNC_MAP = {
#     HUMAN: extract_yes_or_no
# }

# def extract_yes_or_no_(text):
#     flag = 999
#     try:
#         lower_text = text.lower()
#         words = lower_text.split()
#         first_word = words[0].strip(''':/,."' ''')
#         if first_word.startswith('yes'):
#             flag = 1
#         if first_word.startswith('no'):
#             flag = 0
#         if first_word.startswith('harmful'):
#             flag = 1
#         if first_word.startswith('benign'):
#             flag = 0    
#         if flag != 999:
#             output = lower_text.replace(first_word, "").strip(''':/,."' ''')
#         else:
#             output = text
#     except:
#         output = text
#     return {'flag': flag, 'output': output}
