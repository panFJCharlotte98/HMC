{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a3b9b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import regex\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "053a1995",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCHEMES = ['GPT', 'B1','B2', 'PP', 'PL', 'PD']\n",
    "LMMs = ['llava1.6-7bf', 'qwen2-vl-7bf', 'llava1.6-7bf,qwen2-vl-7bf']\n",
    "LLMs = ['qwen2.5-14bf', 'mistral-12bf', 'qwen3-14bf', 'qwen2.5-7bf', 'llama3.1-8bf']\n",
    "MINI = ['llava1.6-7bf', 'qwen2-vl-7bf', 'mistral-12bf', 'qwen2.5-7bf', 'llama3.1-8bf']\n",
    "GPT = ['gpt4o-mini']\n",
    "TASKS = {\n",
    "    'fhm': 'FHM', \n",
    "    'harmc': \"HarMeme\", \n",
    "    'harmp': \"Harm-P\",\n",
    "    'multioff': \"MultiOFF\",\n",
    "    'mami': 'MAMI',\n",
    "    'pridemm': \"PrideMM\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecffdaf",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d50c7c72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seed</th>\n",
       "      <th>split</th>\n",
       "      <th>BS</th>\n",
       "      <th>scheme</th>\n",
       "      <th>LLM</th>\n",
       "      <th>LMM</th>\n",
       "      <th>fhm_Acc</th>\n",
       "      <th>fhm_F1</th>\n",
       "      <th>harmc_Acc</th>\n",
       "      <th>harmc_F1</th>\n",
       "      <th>harmp_Acc</th>\n",
       "      <th>harmp_F1</th>\n",
       "      <th>multioff_Acc</th>\n",
       "      <th>multioff_F1</th>\n",
       "      <th>mami_Acc</th>\n",
       "      <th>mami_F1</th>\n",
       "      <th>pridemm_Acc</th>\n",
       "      <th>pridemm_F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>42</td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>GPT</td>\n",
       "      <td></td>\n",
       "      <td>gpt4o-mini</td>\n",
       "      <td>67.6</td>\n",
       "      <td>65.51</td>\n",
       "      <td>70.9</td>\n",
       "      <td>69.46</td>\n",
       "      <td>65.35</td>\n",
       "      <td>65.35</td>\n",
       "      <td>65.77</td>\n",
       "      <td>64.86</td>\n",
       "      <td>77.4</td>\n",
       "      <td>76.59</td>\n",
       "      <td>72.39</td>\n",
       "      <td>72.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>42</td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>B1</td>\n",
       "      <td></td>\n",
       "      <td>qwen2-vl-7bf</td>\n",
       "      <td>64.2</td>\n",
       "      <td>62.68</td>\n",
       "      <td>67.51</td>\n",
       "      <td>60.29</td>\n",
       "      <td>56.34</td>\n",
       "      <td>53.05</td>\n",
       "      <td>71.14</td>\n",
       "      <td>64.61</td>\n",
       "      <td>68.1</td>\n",
       "      <td>66.03</td>\n",
       "      <td>68.44</td>\n",
       "      <td>68.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>42</td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>B1</td>\n",
       "      <td></td>\n",
       "      <td>llava1.6-7bf</td>\n",
       "      <td>60.4</td>\n",
       "      <td>57.85</td>\n",
       "      <td>66.38</td>\n",
       "      <td>61.05</td>\n",
       "      <td>56.34</td>\n",
       "      <td>53.62</td>\n",
       "      <td>59.73</td>\n",
       "      <td>57.38</td>\n",
       "      <td>67.8</td>\n",
       "      <td>67.46</td>\n",
       "      <td>60.16</td>\n",
       "      <td>59.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>42</td>\n",
       "      <td>test</td>\n",
       "      <td>16</td>\n",
       "      <td>B2</td>\n",
       "      <td>qwen2.5-14bf</td>\n",
       "      <td>qwen2-vl-7bf</td>\n",
       "      <td>70.1</td>\n",
       "      <td>70.02</td>\n",
       "      <td>61.02</td>\n",
       "      <td>57.92</td>\n",
       "      <td>60.0</td>\n",
       "      <td>59.91</td>\n",
       "      <td>53.69</td>\n",
       "      <td>53.15</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>68.44</td>\n",
       "      <td>68.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>42</td>\n",
       "      <td>test</td>\n",
       "      <td>16</td>\n",
       "      <td>B2</td>\n",
       "      <td>qwen2.5-14bf</td>\n",
       "      <td>llava1.6-7bf</td>\n",
       "      <td>68.0</td>\n",
       "      <td>67.7</td>\n",
       "      <td>59.6</td>\n",
       "      <td>51.46</td>\n",
       "      <td>63.94</td>\n",
       "      <td>63.84</td>\n",
       "      <td>64.43</td>\n",
       "      <td>62.93</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>66.47</td>\n",
       "      <td>66.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>42</td>\n",
       "      <td>test</td>\n",
       "      <td>16</td>\n",
       "      <td>PP</td>\n",
       "      <td>qwen2.5-14bf</td>\n",
       "      <td>qwen2-vl-7bf</td>\n",
       "      <td>72.5</td>\n",
       "      <td>72.41</td>\n",
       "      <td>81.92</td>\n",
       "      <td>81.0</td>\n",
       "      <td>65.35</td>\n",
       "      <td>65.35</td>\n",
       "      <td>63.09</td>\n",
       "      <td>62.41</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>70.41</td>\n",
       "      <td>70.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>42</td>\n",
       "      <td>test</td>\n",
       "      <td>16</td>\n",
       "      <td>PP</td>\n",
       "      <td>qwen2.5-14bf</td>\n",
       "      <td>llava1.6-7bf</td>\n",
       "      <td>71.5</td>\n",
       "      <td>71.48</td>\n",
       "      <td>83.62</td>\n",
       "      <td>82.0</td>\n",
       "      <td>63.94</td>\n",
       "      <td>63.67</td>\n",
       "      <td>69.13</td>\n",
       "      <td>68.37</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>71.6</td>\n",
       "      <td>71.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>42</td>\n",
       "      <td>test</td>\n",
       "      <td>16</td>\n",
       "      <td>PL</td>\n",
       "      <td>qwen2.5-14bf</td>\n",
       "      <td>qwen2-vl-7bf</td>\n",
       "      <td>70.5</td>\n",
       "      <td>70.49</td>\n",
       "      <td>67.23</td>\n",
       "      <td>65.9</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>42</td>\n",
       "      <td>test</td>\n",
       "      <td>16</td>\n",
       "      <td>PL</td>\n",
       "      <td>qwen2.5-14bf</td>\n",
       "      <td>llava1.6-7bf</td>\n",
       "      <td>67.2</td>\n",
       "      <td>66.98</td>\n",
       "      <td>63.56</td>\n",
       "      <td>57.14</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>42</td>\n",
       "      <td>test</td>\n",
       "      <td>16</td>\n",
       "      <td>B2</td>\n",
       "      <td>qwen2.5-14bf</td>\n",
       "      <td>llava1.6-7bf,qwen2-vl-7bf</td>\n",
       "      <td>69.1</td>\n",
       "      <td>68.83</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>42</td>\n",
       "      <td>test</td>\n",
       "      <td>16</td>\n",
       "      <td>PP</td>\n",
       "      <td>qwen2.5-14bf</td>\n",
       "      <td>llava1.6-7bf,qwen2-vl-7bf</td>\n",
       "      <td>72.0</td>\n",
       "      <td>71.98</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>42</td>\n",
       "      <td>test</td>\n",
       "      <td>16</td>\n",
       "      <td>PL</td>\n",
       "      <td>qwen2.5-14bf</td>\n",
       "      <td>llava1.6-7bf,qwen2-vl-7bf</td>\n",
       "      <td>68.8</td>\n",
       "      <td>68.68</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   seed split  BS scheme           LLM                        LMM fhm_Acc  \\\n",
       "0    42  test   1    GPT                               gpt4o-mini    67.6   \n",
       "1    42  test   1     B1                             qwen2-vl-7bf    64.2   \n",
       "2    42  test   1     B1                             llava1.6-7bf    60.4   \n",
       "3    42  test  16     B2  qwen2.5-14bf               qwen2-vl-7bf    70.1   \n",
       "4    42  test  16     B2  qwen2.5-14bf               llava1.6-7bf    68.0   \n",
       "5    42  test  16     PP  qwen2.5-14bf               qwen2-vl-7bf    72.5   \n",
       "6    42  test  16     PP  qwen2.5-14bf               llava1.6-7bf    71.5   \n",
       "7    42  test  16     PL  qwen2.5-14bf               qwen2-vl-7bf    70.5   \n",
       "8    42  test  16     PL  qwen2.5-14bf               llava1.6-7bf    67.2   \n",
       "9    42  test  16     B2  qwen2.5-14bf  llava1.6-7bf,qwen2-vl-7bf    69.1   \n",
       "10   42  test  16     PP  qwen2.5-14bf  llava1.6-7bf,qwen2-vl-7bf    72.0   \n",
       "11   42  test  16     PL  qwen2.5-14bf  llava1.6-7bf,qwen2-vl-7bf    68.8   \n",
       "\n",
       "   fhm_F1 harmc_Acc harmc_F1 harmp_Acc harmp_F1 multioff_Acc multioff_F1  \\\n",
       "0   65.51      70.9    69.46     65.35    65.35        65.77       64.86   \n",
       "1   62.68     67.51    60.29     56.34    53.05        71.14       64.61   \n",
       "2   57.85     66.38    61.05     56.34    53.62        59.73       57.38   \n",
       "3   70.02     61.02    57.92      60.0    59.91        53.69       53.15   \n",
       "4    67.7      59.6    51.46     63.94    63.84        64.43       62.93   \n",
       "5   72.41     81.92     81.0     65.35    65.35        63.09       62.41   \n",
       "6   71.48     83.62     82.0     63.94    63.67        69.13       68.37   \n",
       "7   70.49     67.23     65.9                                               \n",
       "8   66.98     63.56    57.14                                               \n",
       "9   68.83                                                                  \n",
       "10  71.98                                                                  \n",
       "11  68.68                                                                  \n",
       "\n",
       "   mami_Acc mami_F1 pridemm_Acc pridemm_F1  \n",
       "0      77.4   76.59       72.39      72.28  \n",
       "1      68.1   66.03       68.44      68.43  \n",
       "2      67.8   67.46       60.16      59.99  \n",
       "3                         68.44      68.42  \n",
       "4                         66.47      66.47  \n",
       "5                         70.41      70.04  \n",
       "6                          71.6      71.37  \n",
       "7                                           \n",
       "8                                           \n",
       "9                                           \n",
       "10                                          \n",
       "11                                          "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def format_number(num):\n",
    "    return str(round(num * 100, 2))\n",
    "\n",
    "def traverse_model_dir(model, model_dir, task, split='test', res_by_task=None, seed=42, batch=1, llm=\"\", lmm=\"\"):\n",
    "    for run in os.listdir(model_dir):\n",
    "        tmp = run.split(\"_\")[0]\n",
    "        if tmp in SCHEMES:\n",
    "            scheme = tmp\n",
    "            if scheme == 'B1':\n",
    "                lmm = model\n",
    "            elif scheme in ['B2', 'PP', 'PL', 'PD']:\n",
    "                llm = model\n",
    "                #lmm = \",\".join(run.split(\"_len-\")[0].split(\"_\")[1:])\n",
    "            run_fd = os.path.join(model_dir, run)\n",
    "            for round in os.listdir(run_fd):\n",
    "                if os.path.isdir(os.path.join(run_fd, round)):\n",
    "                    round_fd = os.path.join(run_fd, round)\n",
    "                    if 'result.json' in os.listdir(round_fd):\n",
    "                        res_file = os.path.join(round_fd, 'result.json')\n",
    "                        res_dict = json.load(open(res_file))\n",
    "                        one_res = {\n",
    "                            'seed': seed,\n",
    "                            'split': split,\n",
    "                            'BS': batch,\n",
    "                            'scheme': scheme,\n",
    "                            'LLM': llm,\n",
    "                            'LMM': lmm,\n",
    "                            'acc': format_number(res_dict['acc']),\n",
    "                            'f1': format_number(res_dict['f1']),\n",
    "                        }\n",
    "                        res_by_task[task].append(one_res)\n",
    "    return res_by_task\n",
    "\n",
    "def traverse_llm_dir(model, llm_fd, task, split='test', res_by_task=None, seed=42):\n",
    "    for lmm in os.listdir(llm_fd):\n",
    "        if lmm in LMMs + GPT:\n",
    "            lmm_fd = os.path.join(llm_fd, lmm)\n",
    "            for batch in os.listdir(lmm_fd):\n",
    "                if regex.match(r\"BS\\-\\d+\", batch):\n",
    "                    batch_num = int(batch.split(\"-\")[1])\n",
    "                    batch_fd = os.path.join(lmm_fd, batch)\n",
    "                    res_by_task = traverse_model_dir(\n",
    "                        model, batch_fd, task, split,\n",
    "                        res_by_task, seed, batch=batch_num, lmm=lmm)\n",
    "    return res_by_task\n",
    "\n",
    "def main_table(\n",
    "    root = \"./results\"\n",
    "):  \n",
    "    ## Start\n",
    "    res_by_task = {task: [] for task in TASKS}\n",
    "    for task in os.listdir(root):\n",
    "        task_fd = os.path.join(root, task)\n",
    "        for split in os.listdir(task_fd):\n",
    "            split_fd = os.path.join(task_fd, split)\n",
    "            for fd in os.listdir(split_fd):\n",
    "                if fd in MINI: ### Mini models <= 13B\n",
    "                    model = fd\n",
    "                    mini_model_fd = os.path.join(split_fd, fd)\n",
    "                    if model in LMMs:\n",
    "                        res_by_task = traverse_model_dir(model, mini_model_fd, task, split, res_by_task)\n",
    "                    elif model in LLMs:\n",
    "                        res_by_task = traverse_llm_dir(model, mini_model_fd, task, split, res_by_task)\n",
    "                if fd.startswith(\"seed-\"): ### Small models > 13B\n",
    "                    this_seed = fd.split(\"-\")[-1]\n",
    "                    seed_fd = os.path.join(split_fd, fd)\n",
    "                    for model in os.listdir(seed_fd):\n",
    "                        model_fd = os.path.join(seed_fd, model)\n",
    "                        if model in GPT:\n",
    "                            res_by_task = traverse_model_dir(model, model_fd, task, split, res_by_task, seed=this_seed)\n",
    "                        elif model in LLMs:### Small models > 13B\n",
    "                            res_by_task = traverse_llm_dir(model, model_fd, task, split, res_by_task, seed=this_seed)\n",
    "\n",
    "    main_dict = {scheme: [] for scheme in SCHEMES}\n",
    "    fhm_specific = {'B2': [], 'PP': [], 'PL': []}\n",
    "    columns = []\n",
    "    task0 = 'fhm'\n",
    "    for rid, rec in enumerate(res_by_task[task0]):\n",
    "        this_acc, this_f1 = rec.pop('acc'), rec.pop('f1')\n",
    "        metrics = {f'{task0}_Acc': this_acc, f'{task0}_F1': this_f1}\n",
    "        oneline = dict(**rec, **metrics)\n",
    "        task0_split = rec['split']\n",
    "        if task0 == 'fhm':\n",
    "            task0_split = rec['split'].split(\"_\")[0] # test_seen --> test\n",
    "            oneline['split'] = task0_split\n",
    "        # Traverse other tasks\n",
    "        # for task, rec_ls in res_by_task.items():\n",
    "        #     if task != task0:\n",
    "        for task in TASKS:\n",
    "            if task != task0:\n",
    "                rec_ls = res_by_task[task]\n",
    "                rec_found = False\n",
    "                for one_rec in rec_ls: # one_rect = {'seed': 42, 'split':'test', ...,'f1': f1,}\n",
    "                    if not rec_found:\n",
    "                        cond = []\n",
    "                        for k, v in one_rec.items():\n",
    "                            if k not in ['acc', 'f1']:\n",
    "                                # if k == 'split':\n",
    "                                #     cond.append(task0_split == v.split(\"_\")[0])\n",
    "                                # else:\n",
    "                                cond.append(oneline[k] == v)\n",
    "                        if cond and all(cond):\n",
    "                            rec_found = True\n",
    "                            this_task_metrics = {f'{task}_Acc': one_rec['acc'], f'{task}_F1': one_rec['f1']}\n",
    "                            # oneline = dict(**oneline, **this_task_metrics)\n",
    "                if not rec_found:\n",
    "                    this_task_metrics = {f'{task}_Acc': \"\", f'{task}_F1': \"\"}\n",
    "                oneline = dict(**oneline, **this_task_metrics)\n",
    "        if not columns:\n",
    "            columns = list(oneline.keys())\n",
    "        \n",
    "        # FHM specific\n",
    "        if oneline['LMM'] == 'llava1.6-7bf,qwen2-vl-7bf':\n",
    "            fhm_specific[oneline['scheme']].append(list(oneline.values()))\n",
    "        else:\n",
    "            main_dict[oneline['scheme']].append(list(oneline.values()))\n",
    "    \n",
    "    main_tab = []\n",
    "    for scheme, line_ls in main_dict.items():\n",
    "        main_tab.extend(line_ls)\n",
    "    for scheme, line_ls in fhm_specific.items():\n",
    "        main_tab.extend(line_ls)\n",
    "    df = pd.DataFrame(main_tab, columns=columns)\n",
    "    return df\n",
    "\n",
    "main_table()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
