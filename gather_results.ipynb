{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a3b9b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import regex\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "053a1995",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCHEMES = ['GPT', 'B1','B2', 'PP', 'PL', 'PD']\n",
    "LMMs = ['llava1.6-7bf', 'qwen2-vl-7bf', 'llava1.6-7bf,qwen2-vl-7bf']\n",
    "LLMs = ['qwen2.5-14bf', 'mistral-12bf', 'qwen3-14bf', 'qwen2.5-7bf', 'llama3.1-8bf']\n",
    "MINI = ['llava1.6-7bf', 'qwen2-vl-7bf', 'mistral-12bf', 'qwen2.5-7bf', 'llama3.1-8bf']\n",
    "GPT = ['gpt4o-mini']\n",
    "TASKS = {\n",
    "    'fhm': 'FHM', \n",
    "    'harmc': \"HarMeme\", \n",
    "    'harmp': \"Harm-P\",\n",
    "    'multioff': \"MultiOFF\",\n",
    "    'mami': 'MAMI',\n",
    "    'pridemm': \"PrideMM\",\n",
    "    'gb_hateful': \"GB-Hateful\",\n",
    "    'gb_harmful': \"GB-Harmful\",\n",
    "    'gb_offensive': \"GB-Offensive\",\n",
    "    'gb_misogynistic': \"GB-Misogynistic\"\n",
    "}\n",
    "model_name_map = {\n",
    "    'llava1.6-7bf': 'LLaVa1.6-7B',\n",
    "    'qwen2-vl-7bf': 'Qwen2VL-7B',\n",
    "    'llava1.6-7bf,qwen2-vl-7bf': 'LLaVa1.6&Qwen2VL-7B',\n",
    "    'qwen2.5-14bf': 'Qwen2.5-14Bf',\n",
    "    'mistral-12bf': 'Mistral-12Bf',\n",
    "    'qwen3-14bf': 'Qwen3-14Bf',\n",
    "    'qwen2.5-7bf': 'Qwen2.5-7Bf',\n",
    "    'llama3.1-8bf': 'Llama3.1-8Bf',\n",
    "    'gpt4o-mini': 'GPT-4o-mini',\n",
    "    \"\": \"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecffdaf",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d50c7c72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seed</th>\n",
       "      <th>split</th>\n",
       "      <th>BS</th>\n",
       "      <th>scheme</th>\n",
       "      <th>LLM</th>\n",
       "      <th>LMM</th>\n",
       "      <th>FHM_Acc</th>\n",
       "      <th>FHM_F1</th>\n",
       "      <th>HarMeme_Acc</th>\n",
       "      <th>HarMeme_F1</th>\n",
       "      <th>Harm-P_Acc</th>\n",
       "      <th>Harm-P_F1</th>\n",
       "      <th>MultiOFF_Acc</th>\n",
       "      <th>MultiOFF_F1</th>\n",
       "      <th>MAMI_Acc</th>\n",
       "      <th>MAMI_F1</th>\n",
       "      <th>PrideMM_Acc</th>\n",
       "      <th>PrideMM_F1</th>\n",
       "      <th>GB-Hateful_Acc</th>\n",
       "      <th>GB-Hateful_F1</th>\n",
       "      <th>GB-Harmful_Acc</th>\n",
       "      <th>GB-Harmful_F1</th>\n",
       "      <th>GB-Offensive_Acc</th>\n",
       "      <th>GB-Offensive_F1</th>\n",
       "      <th>GB-Misogynistic_Acc</th>\n",
       "      <th>GB-Misogynistic_F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>42</td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>GPT</td>\n",
       "      <td></td>\n",
       "      <td>GPT-4o-mini</td>\n",
       "      <td>67.6</td>\n",
       "      <td>65.51</td>\n",
       "      <td>70.9</td>\n",
       "      <td>69.46</td>\n",
       "      <td>65.35</td>\n",
       "      <td>65.35</td>\n",
       "      <td>65.77</td>\n",
       "      <td>64.86</td>\n",
       "      <td>77.4</td>\n",
       "      <td>76.59</td>\n",
       "      <td>72.39</td>\n",
       "      <td>72.28</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>42</td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>B1</td>\n",
       "      <td></td>\n",
       "      <td>Qwen2VL-7B</td>\n",
       "      <td>64.2</td>\n",
       "      <td>62.68</td>\n",
       "      <td>67.51</td>\n",
       "      <td>60.29</td>\n",
       "      <td>56.34</td>\n",
       "      <td>53.05</td>\n",
       "      <td>71.14</td>\n",
       "      <td>64.61</td>\n",
       "      <td>68.1</td>\n",
       "      <td>66.03</td>\n",
       "      <td>68.44</td>\n",
       "      <td>68.43</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>42</td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>B1</td>\n",
       "      <td></td>\n",
       "      <td>LLaVa1.6-7B</td>\n",
       "      <td>60.4</td>\n",
       "      <td>57.85</td>\n",
       "      <td>66.38</td>\n",
       "      <td>61.05</td>\n",
       "      <td>56.34</td>\n",
       "      <td>53.62</td>\n",
       "      <td>59.73</td>\n",
       "      <td>57.38</td>\n",
       "      <td>67.8</td>\n",
       "      <td>67.46</td>\n",
       "      <td>60.16</td>\n",
       "      <td>59.99</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>42</td>\n",
       "      <td>test</td>\n",
       "      <td>16</td>\n",
       "      <td>B2</td>\n",
       "      <td>Qwen2.5-14Bf</td>\n",
       "      <td>Qwen2VL-7B</td>\n",
       "      <td>70.1</td>\n",
       "      <td>70.02</td>\n",
       "      <td>61.02</td>\n",
       "      <td>57.92</td>\n",
       "      <td>60.0</td>\n",
       "      <td>59.91</td>\n",
       "      <td>53.69</td>\n",
       "      <td>53.15</td>\n",
       "      <td>75.1</td>\n",
       "      <td>75.1</td>\n",
       "      <td>68.44</td>\n",
       "      <td>68.42</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>42</td>\n",
       "      <td>test</td>\n",
       "      <td>16</td>\n",
       "      <td>B2</td>\n",
       "      <td>Qwen2.5-14Bf</td>\n",
       "      <td>LLaVa1.6&amp;Qwen2VL-7B</td>\n",
       "      <td>69.1</td>\n",
       "      <td>68.83</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>42</td>\n",
       "      <td>test</td>\n",
       "      <td>16</td>\n",
       "      <td>B2</td>\n",
       "      <td>Qwen2.5-14Bf</td>\n",
       "      <td>LLaVa1.6-7B</td>\n",
       "      <td>68.0</td>\n",
       "      <td>67.7</td>\n",
       "      <td>59.6</td>\n",
       "      <td>51.46</td>\n",
       "      <td>63.94</td>\n",
       "      <td>63.84</td>\n",
       "      <td>64.43</td>\n",
       "      <td>62.93</td>\n",
       "      <td>76.4</td>\n",
       "      <td>76.38</td>\n",
       "      <td>66.47</td>\n",
       "      <td>66.47</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>42</td>\n",
       "      <td>test</td>\n",
       "      <td>16</td>\n",
       "      <td>PP</td>\n",
       "      <td>Qwen2.5-14Bf</td>\n",
       "      <td>Qwen2VL-7B</td>\n",
       "      <td>72.5</td>\n",
       "      <td>72.41</td>\n",
       "      <td>81.92</td>\n",
       "      <td>81.0</td>\n",
       "      <td>65.35</td>\n",
       "      <td>65.35</td>\n",
       "      <td>63.09</td>\n",
       "      <td>62.41</td>\n",
       "      <td>78.6</td>\n",
       "      <td>78.59</td>\n",
       "      <td>70.41</td>\n",
       "      <td>70.04</td>\n",
       "      <td>71.25</td>\n",
       "      <td>70.75</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>42</td>\n",
       "      <td>test</td>\n",
       "      <td>16</td>\n",
       "      <td>PP</td>\n",
       "      <td>Qwen2.5-14Bf</td>\n",
       "      <td>LLaVa1.6&amp;Qwen2VL-7B</td>\n",
       "      <td>72.0</td>\n",
       "      <td>71.98</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>70.8</td>\n",
       "      <td>69.95</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>42</td>\n",
       "      <td>test</td>\n",
       "      <td>16</td>\n",
       "      <td>PP</td>\n",
       "      <td>Qwen2.5-14Bf</td>\n",
       "      <td>LLaVa1.6-7B</td>\n",
       "      <td>71.5</td>\n",
       "      <td>71.48</td>\n",
       "      <td>83.62</td>\n",
       "      <td>82.0</td>\n",
       "      <td>63.94</td>\n",
       "      <td>63.67</td>\n",
       "      <td>69.13</td>\n",
       "      <td>68.37</td>\n",
       "      <td>79.9</td>\n",
       "      <td>79.89</td>\n",
       "      <td>71.6</td>\n",
       "      <td>71.37</td>\n",
       "      <td>71.95</td>\n",
       "      <td>71.05</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>42</td>\n",
       "      <td>test</td>\n",
       "      <td>16</td>\n",
       "      <td>PL</td>\n",
       "      <td>Qwen2.5-14Bf</td>\n",
       "      <td>Qwen2VL-7B</td>\n",
       "      <td>70.5</td>\n",
       "      <td>70.49</td>\n",
       "      <td>67.23</td>\n",
       "      <td>65.9</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>75.8</td>\n",
       "      <td>75.78</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>42</td>\n",
       "      <td>test</td>\n",
       "      <td>16</td>\n",
       "      <td>PL</td>\n",
       "      <td>Qwen2.5-14Bf</td>\n",
       "      <td>LLaVa1.6&amp;Qwen2VL-7B</td>\n",
       "      <td>68.8</td>\n",
       "      <td>68.68</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>42</td>\n",
       "      <td>test</td>\n",
       "      <td>16</td>\n",
       "      <td>PL</td>\n",
       "      <td>Qwen2.5-14Bf</td>\n",
       "      <td>LLaVa1.6-7B</td>\n",
       "      <td>67.2</td>\n",
       "      <td>66.98</td>\n",
       "      <td>63.56</td>\n",
       "      <td>57.14</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>76.7</td>\n",
       "      <td>76.7</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   seed split  BS scheme           LLM                  LMM FHM_Acc FHM_F1  \\\n",
       "0    42  test   1    GPT                        GPT-4o-mini    67.6  65.51   \n",
       "1    42  test   1     B1                         Qwen2VL-7B    64.2  62.68   \n",
       "2    42  test   1     B1                        LLaVa1.6-7B    60.4  57.85   \n",
       "3    42  test  16     B2  Qwen2.5-14Bf           Qwen2VL-7B    70.1  70.02   \n",
       "4    42  test  16     B2  Qwen2.5-14Bf  LLaVa1.6&Qwen2VL-7B    69.1  68.83   \n",
       "5    42  test  16     B2  Qwen2.5-14Bf          LLaVa1.6-7B    68.0   67.7   \n",
       "6    42  test  16     PP  Qwen2.5-14Bf           Qwen2VL-7B    72.5  72.41   \n",
       "7    42  test  16     PP  Qwen2.5-14Bf  LLaVa1.6&Qwen2VL-7B    72.0  71.98   \n",
       "8    42  test  16     PP  Qwen2.5-14Bf          LLaVa1.6-7B    71.5  71.48   \n",
       "9    42  test  16     PL  Qwen2.5-14Bf           Qwen2VL-7B    70.5  70.49   \n",
       "10   42  test  16     PL  Qwen2.5-14Bf  LLaVa1.6&Qwen2VL-7B    68.8  68.68   \n",
       "11   42  test  16     PL  Qwen2.5-14Bf          LLaVa1.6-7B    67.2  66.98   \n",
       "\n",
       "   HarMeme_Acc HarMeme_F1 Harm-P_Acc Harm-P_F1 MultiOFF_Acc MultiOFF_F1  \\\n",
       "0         70.9      69.46      65.35     65.35        65.77       64.86   \n",
       "1        67.51      60.29      56.34     53.05        71.14       64.61   \n",
       "2        66.38      61.05      56.34     53.62        59.73       57.38   \n",
       "3        61.02      57.92       60.0     59.91        53.69       53.15   \n",
       "4                                                                         \n",
       "5         59.6      51.46      63.94     63.84        64.43       62.93   \n",
       "6        81.92       81.0      65.35     65.35        63.09       62.41   \n",
       "7                                                                         \n",
       "8        83.62       82.0      63.94     63.67        69.13       68.37   \n",
       "9        67.23       65.9                                                 \n",
       "10                                                                        \n",
       "11       63.56      57.14                                                 \n",
       "\n",
       "   MAMI_Acc MAMI_F1 PrideMM_Acc PrideMM_F1 GB-Hateful_Acc GB-Hateful_F1  \\\n",
       "0      77.4   76.59       72.39      72.28                                \n",
       "1      68.1   66.03       68.44      68.43                                \n",
       "2      67.8   67.46       60.16      59.99                                \n",
       "3      75.1    75.1       68.44      68.42                                \n",
       "4                                                                         \n",
       "5      76.4   76.38       66.47      66.47                                \n",
       "6      78.6   78.59       70.41      70.04          71.25         70.75   \n",
       "7                                                    70.8         69.95   \n",
       "8      79.9   79.89        71.6      71.37          71.95         71.05   \n",
       "9      75.8   75.78                                                       \n",
       "10                                                                        \n",
       "11     76.7    76.7                                                       \n",
       "\n",
       "   GB-Harmful_Acc GB-Harmful_F1 GB-Offensive_Acc GB-Offensive_F1  \\\n",
       "0                                                                  \n",
       "1                                                                  \n",
       "2                                                                  \n",
       "3                                                                  \n",
       "4                                                                  \n",
       "5                                                                  \n",
       "6                                                                  \n",
       "7                                                                  \n",
       "8                                                                  \n",
       "9                                                                  \n",
       "10                                                                 \n",
       "11                                                                 \n",
       "\n",
       "   GB-Misogynistic_Acc GB-Misogynistic_F1  \n",
       "0                                          \n",
       "1                                          \n",
       "2                                          \n",
       "3                                          \n",
       "4                                          \n",
       "5                                          \n",
       "6                                          \n",
       "7                                          \n",
       "8                                          \n",
       "9                                          \n",
       "10                                         \n",
       "11                                         "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def format_number(num):\n",
    "    return str(round(num * 100, 2))\n",
    "\n",
    "def traverse_model_dir(model, model_dir, task, split='test', res_by_task=None, seed=42, batch=1, llm=\"\", lmm=\"\"):\n",
    "    for run in os.listdir(model_dir):\n",
    "        tmp = run.split(\"_\")[0]\n",
    "        if tmp in SCHEMES:\n",
    "            scheme = tmp\n",
    "            if scheme == 'B1':\n",
    "                lmm = model\n",
    "            elif scheme in ['B2', 'PP', 'PL', 'PD']:\n",
    "                llm = model\n",
    "                #lmm = \",\".join(run.split(\"_len-\")[0].split(\"_\")[1:])\n",
    "            run_fd = os.path.join(model_dir, run)\n",
    "            for round in os.listdir(run_fd):\n",
    "                if os.path.isdir(os.path.join(run_fd, round)):\n",
    "                    round_fd = os.path.join(run_fd, round)\n",
    "                    if 'result.json' in os.listdir(round_fd):\n",
    "                        res_file = os.path.join(round_fd, 'result.json')\n",
    "                        res_dict = json.load(open(res_file))\n",
    "                        try:\n",
    "                            one_res = {\n",
    "                                'seed': seed,\n",
    "                                'split': split,\n",
    "                                'BS': batch,\n",
    "                                'scheme': scheme,\n",
    "                                'LLM': model_name_map[llm],\n",
    "                                'LMM': model_name_map[lmm],\n",
    "                                'acc': format_number(res_dict['acc']),\n",
    "                                'f1': format_number(res_dict['f1']),\n",
    "                            }\n",
    "                        except:\n",
    "                            print(llm)\n",
    "                            print(lmm)\n",
    "                        res_by_task[task].append(one_res)\n",
    "    return res_by_task\n",
    "\n",
    "def traverse_llm_dir(model, llm_fd, task, split='test', res_by_task=None, seed=42):\n",
    "    for lmm in os.listdir(llm_fd):\n",
    "        if lmm in LMMs + GPT:\n",
    "            lmm_fd = os.path.join(llm_fd, lmm)\n",
    "            for batch in os.listdir(lmm_fd):\n",
    "                if regex.match(r\"BS\\-\\d+\", batch):\n",
    "                    batch_num = int(batch.split(\"-\")[1])\n",
    "                    batch_fd = os.path.join(lmm_fd, batch)\n",
    "                    res_by_task = traverse_model_dir(\n",
    "                        model, batch_fd, task, split,\n",
    "                        res_by_task, seed, batch=batch_num, lmm=lmm)\n",
    "    return res_by_task\n",
    "\n",
    "def main_table(\n",
    "    root = \"./results\"\n",
    "):  \n",
    "    ## Start\n",
    "    res_by_task = {task: [] for task in TASKS}\n",
    "    for task in os.listdir(root):\n",
    "        task_fd = os.path.join(root, task)\n",
    "        for split in os.listdir(task_fd):\n",
    "            split_fd = os.path.join(task_fd, split)\n",
    "            for fd in os.listdir(split_fd):\n",
    "                if fd in MINI: ### Mini models <= 13B\n",
    "                    model = fd\n",
    "                    mini_model_fd = os.path.join(split_fd, fd)\n",
    "                    if model in LMMs:\n",
    "                        res_by_task = traverse_model_dir(model, mini_model_fd, task, split, res_by_task)\n",
    "                    elif model in LLMs:\n",
    "                        res_by_task = traverse_llm_dir(model, mini_model_fd, task, split, res_by_task)\n",
    "                if fd.startswith(\"seed-\"): ### Small models > 13B\n",
    "                    this_seed = fd.split(\"-\")[-1]\n",
    "                    seed_fd = os.path.join(split_fd, fd)\n",
    "                    for model in os.listdir(seed_fd):\n",
    "                        model_fd = os.path.join(seed_fd, model)\n",
    "                        if model in GPT:\n",
    "                            res_by_task = traverse_model_dir(model, model_fd, task, split, res_by_task, seed=this_seed)\n",
    "                        elif model in LLMs:### Small models > 13B\n",
    "                            res_by_task = traverse_llm_dir(model, model_fd, task, split, res_by_task, seed=this_seed)\n",
    "\n",
    "    main_dict = {scheme: [] for scheme in SCHEMES}\n",
    "    fhm_specific = {'B2': [], 'PP': [], 'PL': []}\n",
    "    columns = []\n",
    "    task0 = 'fhm'\n",
    "    for rid, rec in enumerate(res_by_task[task0]):\n",
    "        this_acc, this_f1 = rec.pop('acc'), rec.pop('f1')\n",
    "        task0_display = TASKS[task0]\n",
    "        metrics = {f'{task0_display}_Acc': this_acc, f'{task0_display}_F1': this_f1}\n",
    "        oneline = dict(**rec, **metrics)\n",
    "        task0_split = rec['split']\n",
    "        if task0 == 'fhm':\n",
    "            task0_split = rec['split'].split(\"_\")[0] # test_seen --> test\n",
    "            oneline['split'] = task0_split\n",
    "        # Traverse other tasks\n",
    "        # for task, rec_ls in res_by_task.items():\n",
    "        #     if task != task0:\n",
    "        for task in TASKS:\n",
    "            if task != task0:\n",
    "                task_display = TASKS[task]\n",
    "                rec_ls = res_by_task[task]\n",
    "                rec_found = False\n",
    "                for one_rec in rec_ls: # one_rect = {'seed': 42, 'split':'test', ...,'f1': f1,}\n",
    "                    if not rec_found:\n",
    "                        cond = []\n",
    "                        for k, v in one_rec.items():\n",
    "                            if k not in ['acc', 'f1']:\n",
    "                                # if k == 'split':\n",
    "                                #     cond.append(task0_split == v.split(\"_\")[0])\n",
    "                                # else:\n",
    "                                cond.append(oneline[k] == v)\n",
    "                        if cond and all(cond):\n",
    "                            rec_found = True\n",
    "                            this_task_metrics = {f'{task_display}_Acc': one_rec['acc'], f'{task_display}_F1': one_rec['f1']}\n",
    "                            # oneline = dict(**oneline, **this_task_metrics)\n",
    "                if not rec_found:\n",
    "                    this_task_metrics = {f'{task_display}_Acc': \"\", f'{task_display}_F1': \"\"}\n",
    "                oneline = dict(**oneline, **this_task_metrics)\n",
    "        if not columns:\n",
    "            columns = list(oneline.keys())\n",
    "        \n",
    "        # FHM specific\n",
    "        if oneline['LMM'] == 'llava1.6-7bf,qwen2-vl-7bf':\n",
    "            fhm_specific[oneline['scheme']].append(list(oneline.values()))\n",
    "        else:\n",
    "            main_dict[oneline['scheme']].append(list(oneline.values()))\n",
    "    \n",
    "    main_tab = []\n",
    "    for scheme, line_ls in main_dict.items():\n",
    "        main_tab.extend(line_ls)\n",
    "    for scheme, line_ls in fhm_specific.items():\n",
    "        main_tab.extend(line_ls)\n",
    "    df = pd.DataFrame(main_tab, columns=columns)\n",
    "    save_to = \"./res_tables\"\n",
    "    if not os.path.exists(save_to):\n",
    "        os.makedirs(save_to)\n",
    "    df.to_excel(os.path.join(save_to, \"main208.xlsx\"), sheet_name='208', index=False) \n",
    "    return df\n",
    "\n",
    "main_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088ad200",
   "metadata": {},
   "source": [
    "### Confusion Matrices (B1 vs. B2 vs. PP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151ec042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qwen2VL-7B: B1, B2, PP\n",
    "#FHM\n",
    "#Harm-C\n",
    "#Harm-P\n",
    "#MultiOFF\n",
    "#MAMI\n",
    "#PrideMM\n",
    "Qwen2VL = [\n",
    "    [[[422,88],[270,220]], [[377,133],[166,324]], [[334,176],[99,391]]],\n",
    "    [[[195,35],[80,44]], [[156,74],[64,60]], [[184,46],[18,106]]],\n",
    "    [[[147,37],[118,53]], [[98,  86],[56, 115]], [[117,67],[56,115]]],\n",
    "    [[[85,6],[37, 21]], [[48, 43],[26, 32]], [[57, 34],[21, 37]]],\n",
    "    [[[464,36],[283, 217]], [[378, 122],[127, 373]], [[406,  94],[120,380]]],\n",
    "    [[[178,82],[78,169]], [[180,80],[80,167]], [[207, 53],[97, 150]]] \n",
    "]\n",
    "\n",
    "# Llava1.6-7B: B1, B2, PP\n",
    "Llava = [\n",
    "    [[[425, 85],[311, 179]], [[388, 122],[198, 292]], [[343, 167],[118, 372]]],\n",
    "    [[[183,  47],[72,  52]], [[178,  52],[91,  33]], [[201,  29],[29,  95]]],\n",
    "    [[[143, 41],[114, 57]], [[123,  61],[67, 104]], [[129,  55],[73,  98]]],\n",
    "    [[[62, 29],[31, 27]], [[63, 28],[25, 33]], [[63, 28],[18, 40]]],\n",
    "    [[[390, 110],[212, 288]], [[367, 133],[103, 397]], [[387, 113],[88, 412]]],\n",
    "    [[[136, 124],[78, 169]], [[166,  94],[76, 171]], [[204,  56],[88, 159]]]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5e46bb",
   "metadata": {},
   "source": [
    "### Iterative Improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cd7ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FHM\n",
    "FHM = {\n",
    "    'l1': [68.38, 70.18, 70.64, 70.99, 71.68, 72.08],\n",
    "    'l2': [65.51] * 6\n",
    "}\n",
    "# HarMeme\n",
    "HarMeme = {\n",
    "    'l1': [77.24, 75.91, 79.94, 78.36, 81.31, 80.35],\n",
    "    'l2': [69.46] * 6,\n",
    "}\n",
    "# HarmP\n",
    "HarmP = {\n",
    "    'l1': [57.67, 61.65, 63.08, 62.2, 61.96, 63.09],\n",
    "    'l2': [65.35] * 6,\n",
    "}\n",
    "# MultiOFF\n",
    "MultiOFF = {\n",
    "    'l1': [59.65, 62.71, 61.89, 63.64, 61.93, 66.85],\n",
    "    'l2': [64.86] * 6,\n",
    "} \n",
    "# MAMI\n",
    "MAMI = {\n",
    "    'l1': [77.18, 78.6, 78.4, 79.6, 79.3, 79.59],\n",
    "    'l2': [76.59] * 6,\n",
    "}\n",
    "# PrideMM\n",
    "PrideMM = {\n",
    "    'l1': [68.24, 70.02, 68.83, 70.45, 70.92, 71.11],\n",
    "    'l2': [72.28] * 6,\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
