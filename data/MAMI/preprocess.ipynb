{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd    \n",
    "import pprint\n",
    "import shutil\n",
    "import random\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GoatBench Loaded ...\n",
      "test finished! #data = 1000\n",
      "dev finished! #data = 1000\n",
      "{'dev': {'misogynistic': 500,\n",
      "         'non-misogynistic': 500,\n",
      "         'objectification': 228,\n",
      "         'shaming': 123,\n",
      "         'stereotype': 276,\n",
      "         'violence': 100},\n",
      " 'test': {'misogynistic': 500,\n",
      "          'non-misogynistic': 500,\n",
      "          'objectification': 348,\n",
      "          'shaming': 146,\n",
      "          'stereotype': 350,\n",
      "          'violence': 153}}\n",
      "#GOATBENCH Instances = 1000 = 1000\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    text = text.replace(\"Meme Center\", \"\")\n",
    "    pttns = [\".net\", \".co\", \".com\", \".c\"]\n",
    "    new_word_ls = []\n",
    "    for w in text.split():\n",
    "        wl = w.lower()\n",
    "        if not any([wl.strip(\"]/*@.,!#$^()%-;: \").endswith(p) for p in pttns]):\n",
    "            new_word_ls.append(w)\n",
    "    text = \" \".join(new_word_ls)\n",
    "    return text\n",
    "        \n",
    "def preprocess_MAMI(\n",
    "    root=\"./\",\n",
    "    save_data_jsons=False,\n",
    "    include_gt_captions=False\n",
    "):\n",
    "    # # Process Goatbench\n",
    "    gb_count = 0\n",
    "    gb_folder = \"../../data/GoatBench/misogyny/\"\n",
    "    GB_items = {}\n",
    "    new_GB_items = {}\n",
    "    if os.path.exists(gb_folder):\n",
    "        gb_path = os.path.join(gb_folder, \"test.jsonl\")\n",
    "        f = open(gb_path, 'r')\n",
    "        for line in f:\n",
    "            one_data = json.loads(line)\n",
    "            if one_data[\"id\"] not in GB_items:\n",
    "                GB_items[one_data[\"id\"]] = {'label': int(one_data[\"label\"]), 'text': one_data[\"text\"]}\n",
    "            else:\n",
    "                print(\"redundancy detected ...\")\n",
    "        gb_save_to = \"../../data/GoatBench/data/misogyny/\"\n",
    "        if not os.path.exists(gb_save_to):\n",
    "            os.makedirs(gb_save_to)\n",
    "        print(\"GoatBench Loaded ...\")\n",
    "\n",
    "    save_to = os.path.join(root, 'data')\n",
    "    if not os.path.exists(save_to):\n",
    "        os.makedirs(save_to)\n",
    "    # root = os.path.join(root, 'src')\n",
    "    image_path = os.path.join(root, 'MAMI_2022_images')\n",
    "\n",
    "    img_dist = {}\n",
    "    for sp in ['training', 'test']:\n",
    "        img_dist[sp] = []\n",
    "        if os.path.exists(os.path.join(image_path, f\"{sp}_images\")):\n",
    "            for img in os.listdir(os.path.join(image_path, f\"{sp}_images\")):\n",
    "                img_dist[sp].append(img.split(\".\")[0])\n",
    "    splits = ['test', 'dev']\n",
    "    categories = ['misogynistic', 'non-misogynistic']\n",
    "    sub_categories = ['shaming', 'stereotype', 'objectification', 'violence']\n",
    "    #ddict = {t: 0 for t in categories + sub_categories}\n",
    "    data_statistics = {sp: {'misogynistic':0, 'non-misogynistic':0, 'shaming':0, 'stereotype':0, 'objectification':0, 'violence':0} for sp in splits}\n",
    "    #print(data_statistics)\n",
    "\n",
    "    for split in splits:\n",
    "        sp = split\n",
    "        if include_gt_captions:\n",
    "            annotations = {}\n",
    "            anno_file = os.path.join(f\"./caption_annotations\", f\"{split}.json\")\n",
    "            if os.path.exists(anno_file):\n",
    "                annotations = json.load(anno_file) # a dict {\"12\": {'img':, 'gt_caption':}}\n",
    "        if split == 'dev':\n",
    "            sp = 'validation'\n",
    "        \n",
    "        new_data = []\n",
    "        file_path = os.path.join(root, f\"{sp}.tsv\")\n",
    "        split_df = pd.read_csv(file_path, sep='\\t')\n",
    "        result = split_df.to_json(orient=\"records\")\n",
    "        ori_data = json.loads(result)\n",
    "        \n",
    "        for item in ori_data:\n",
    "            # remove img\n",
    "            new_id = item['file_name'].split(\".\")[0]\n",
    "            item['id'] = new_id\n",
    "            item.pop('file_name')\n",
    "            if item['label'] == 1:\n",
    "                data_statistics[split]['misogynistic'] += 1\n",
    "            else:\n",
    "                data_statistics[split]['non-misogynistic'] += 1\n",
    "            \n",
    "            item[\"sub_label\"] = []\n",
    "            for t in sub_categories:\n",
    "                if item[t] == 1:\n",
    "                    item[\"sub_label\"].append(t)\n",
    "                    data_statistics[split][t] += 1\n",
    "                    item.pop(t)\n",
    "                else:\n",
    "                    item.pop(t)\n",
    "            \n",
    "            if include_gt_captions:\n",
    "                item['gt_description'] = \"\"\n",
    "                if annotations and (item[\"id\"] in annotations):\n",
    "                    item['gt_description'] = annotations[item[\"id\"]][\"gt_caption\"]\n",
    "            \n",
    "            # -------------------------- copy and move imgage -------------------------- #\n",
    "            label_folder = item[\"sub_label\"][-1] if item[\"sub_label\"] else \"0\"\n",
    "            new_split_image_path = os.path.join(image_path, split, label_folder)\n",
    "            if not os.path.exists(new_split_image_path):\n",
    "                os.makedirs(new_split_image_path)\n",
    "            new_img_path = os.path.join(image_path, split, label_folder, f\"{new_id}.png\")\n",
    "            if not os.path.exists(new_img_path):\n",
    "                ori_img_path = \"\"\n",
    "                for sp_dir, img_ls in img_dist.items():\n",
    "                    if new_id in img_ls:\n",
    "                        ori_img_path = os.path.join(image_path, f\"{sp_dir}_images\", f\"{new_id}.jpg\")\n",
    "                        assert os.path.exists(ori_img_path)\n",
    "                        break\n",
    "                assert ori_img_path != \"\"\n",
    "                #os.rename(this_img_path, new_img_path)\n",
    "                shutil.copy(ori_img_path, new_img_path)\n",
    "\n",
    "            item['img'] = os.path.join('./data/MAMI', 'MAMI_2022_images', split, label_folder, f\"{new_id}.png\")\n",
    "            # -------------------------- copy and move imgage -------------------------- #\n",
    "            caption = item.pop(\"text\")\n",
    "            item[\"text\"] = clean_text(caption)\n",
    "            new_data.append(item)\n",
    "            if GB_items:\n",
    "                if new_id in list(GB_items.keys()):\n",
    "                    ori_item = copy.deepcopy(GB_items[new_id])\n",
    "                    ori_text = ori_item[\"text\"]\n",
    "                    ori_text = clean_text(ori_text)\n",
    "                    if set(item['text'].split()) <= set(ori_text.split()):\n",
    "                        if new_id not in new_GB_items:\n",
    "                            gb_count += 1\n",
    "                            one_gb = copy.deepcopy(item)\n",
    "                            one_gb.pop(\"label\")\n",
    "                            ori_item.pop(\"text\")\n",
    "                            one_gb[\"task\"] = \"mami\"\n",
    "                            new_GB_items[new_id] = dict(**ori_item, **one_gb)\n",
    "                            if split in [\"train\", \"dev\"]:\n",
    "                                print(f\"one {split} entry...\")\n",
    "                    else:\n",
    "                        print(\"same id but different caption ...\")\n",
    "                        clean_ori_text = item['text']\n",
    "                        print(f\"sp: {split}| id: {new_id}\\nori caption = {clean_ori_text}\\n goatbench caption = {ori_text}\\n********************\")\n",
    "                        #break\n",
    "        \n",
    "        if save_data_jsons:\n",
    "            json.dump(new_data, open(os.path.join(save_to, f'{split}.json'), 'w'), indent=4)\n",
    "        print(f\"{split} finished! #data = {len(new_data)}\")\n",
    "        #print(pprint.pformat(data_statistics))\n",
    "    print(pprint.pformat(data_statistics))\n",
    "    if save_data_jsons and new_GB_items:\n",
    "        print(f\"#GOATBENCH Instances = {gb_count} = {len(new_GB_items)}\")\n",
    "        json.dump([item for _, item in new_GB_items.items()], open(os.path.join(gb_save_to, f'test.json'), 'w'), indent=4)\n",
    "\n",
    "preprocess_MAMI(save_data_jsons=True, include_gt_captions=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate annotation sheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#data to be annotated in test: 310\n"
     ]
    }
   ],
   "source": [
    "def mami_gen_annotation_sheet(source_paths):\n",
    "    sep = \"mami/\"\n",
    "    save_to = \"./caption_annotations\"\n",
    "    if not os.path.exists(save_to):\n",
    "        os.makedirs(save_to)\n",
    "    splits = list(set([src.split(sep)[1].split(\"/\")[0] for src in source_paths]))\n",
    "    splits_save_to = {sp: os.path.join(save_to, f'{sp}.json') for sp in splits}\n",
    "    #data_to_annotate = {sp: {} for sp in splits}\n",
    "    data_to_annotate = {}\n",
    "    for sp, path in splits_save_to.items():\n",
    "        if os.path.exists(path):\n",
    "            data_to_annotate[sp] = json.load(open(path))\n",
    "        else:\n",
    "            data_to_annotate[sp] = {}\n",
    "    for src in source_paths:\n",
    "        split = src.split(sep)[1].split(\"/\")[0]\n",
    "        for item in json.load(open(src)):\n",
    "            if item['id'] not in data_to_annotate[split]:\n",
    "                data_to_annotate[split][item['id']] = {\n",
    "                    'img': item['img'],\n",
    "                    'gt_caption': ''''''\n",
    "                }\n",
    "    for sp, data in data_to_annotate.items():\n",
    "        json.dump(data, open(splits_save_to[sp], 'w'), indent=4)\n",
    "        print(f\"#data to be annotated in {sp}: {len(data)}\")\n",
    "    return\n",
    "\n",
    "sources = [\n",
    "    \"/data/fengjun/projects/LLM/meme/HMC/results/mami/test/seed-42/qwen2.5-14bf/D6_llava1.6-7bf_len-1024_GPU-2_20250316015122/best/79.91/round-1_StepDecision-vio-vio/incorrect.json\",\n",
    "    \"/data/fengjun/projects/LLM/meme/HMC/results/mami/test/seed-42/qwen2.5-14bf/D6_llava1.6-7bf_len-1024_GPU-2_20250316015122/best/79.4/round-1_StepDecision-vio-vio/incorrect.json\",\n",
    "    \"/data/fengjun/projects/LLM/meme/HMC/results/mami/test/seed-42/qwen2.5-14bf/D6_llava1.6-7bf_len-1024_GPU-2_20250316015122/history/1-round-1_Decision-v0-v0/incorrect.json\"\n",
    "]\n",
    "mami_gen_annotation_sheet(sources)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
